{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DoubleType,StructField, StructType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://synclapn17855:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>partitioning_shuffle</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xa1c0e90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local[3]\").setAppName(\"partitioning_shuffle\").set(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.11:0.12.2\")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    ".option(\"useHeader\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"C:\\\\Users\\\\EdwinVivekN\\\\Desktop\\\\sample files\\\\test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".csv(\"C:\\\\Users\\\\EdwinVivekN\\Desktop\\\\sample files\\\\NEW_All_AppleTV_PrimeVideo_RevenueTrackin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: string (nullable = true)\n",
      " |-- Title/Season ID: string (nullable = true)\n",
      " |-- ASIN: string (nullable = true)\n",
      " |-- Title Name: string (nullable = true)\n",
      " |-- Series Name: string (nullable = true)\n",
      " |-- Season Name: string (nullable = true)\n",
      " |-- Consumption Type: string (nullable = true)\n",
      " |-- Quality: string (nullable = true)\n",
      " |-- Unit Price: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Duration Streamed: string (nullable = true)\n",
      " |-- Paid Ad Impressions: string (nullable = true)\n",
      " |-- Ad Revenue: string (nullable = true)\n",
      " |-- Royalty Rate: double (nullable = true)\n",
      " |-- Royalty Amount: string (nullable = true)\n",
      " |-- Royalty Currency: string (nullable = true)\n",
      " |-- Exchange rate: string (nullable = true)\n",
      " |-- $USD Royalty Amount: string (nullable = true)\n",
      " |--  Estimated $USD GROSS revenues : string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Territory: string (nullable = true)\n",
      " |-- Period Start: string (nullable = true)\n",
      " |-- Period End: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- CER Percentile: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7962"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        Unit Price|\n",
      "+-------+------------------+\n",
      "|  count|              4515|\n",
      "|   mean|5.5489700996681455|\n",
      "| stddev| 3.577174063761296|\n",
      "|    min|               0.0|\n",
      "|    max|             15.99|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Row ID: double, Title/Season ID: string, ASIN: string, Title Name: string, Series Name: string, Season Name: string, Consumption Type: string, Quality: string, Unit Price: double, Quantity: int, Duration Streamed: string, Paid Ad Impressions: string, Ad Revenue: string, Royalty Rate: double, Royalty Amount: string, Royalty Currency: string, Exchange rate: string, $USD Royalty Amount: string,  Estimated $USD GROSS revenues : string, Region: string, Territory: string, Period Start: string, Period End: string, Type: string, CER Percentile: string, _c25: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.count()\n",
    "df.describe('Unit Price').show()\n",
    "\n",
    "\n",
    "#change data type\n",
    "changedTypedf = df.withColumn(\"Row ID\", df[\"Row ID\"].cast(DoubleType()))\n",
    "changedTypedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|         Colunm Name|Distinct Count|\n",
      "+--------------------+--------------+\n",
      "|              Row ID|          7962|\n",
      "|     Title/Season ID|           266|\n",
      "|                ASIN|          1104|\n",
      "|          Title Name|           255|\n",
      "|         Series Name|             5|\n",
      "|         Season Name|             5|\n",
      "|    Consumption Type|             3|\n",
      "|             Quality|             3|\n",
      "|          Unit Price|            88|\n",
      "|            Quantity|           326|\n",
      "|   Duration Streamed|          2793|\n",
      "| Paid Ad Impressions|             1|\n",
      "|          Ad Revenue|             1|\n",
      "|        Royalty Rate|             4|\n",
      "|      Royalty Amount|          1719|\n",
      "|    Royalty Currency|             4|\n",
      "|       Exchange rate|            25|\n",
      "| $USD Royalty Amount|          2574|\n",
      "| Estimated $USD G...|          2369|\n",
      "|              Region|             5|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct count of 1 field\n",
    "\n",
    "df.select('Quality').distinct().count()\n",
    "\n",
    "\n",
    "#Distinct count of all fields\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    rows.append([c, df.select(c).distinct().count()])\n",
    "df2 = spark.createDataFrame(rows, [\"Colunm Name\", \"Distinct Count\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|Title/Season ID|count(Row ID)|\n",
      "+---------------+-------------+\n",
      "|    8Z1Q3WNG809|          152|\n",
      "|    T88GM85KJ6M|           30|\n",
      "|    8QCXQQ7S13K|           28|\n",
      "|    RB0662ZDVHD|           14|\n",
      "|    GQ08HR5MC5H|            3|\n",
      "|    ESSAG5226FC|            4|\n",
      "|    63KGWNZ6PCV|            3|\n",
      "|    T6DMTK8PGZJ|            7|\n",
      "|    T7HWTHBJD56|           28|\n",
      "|    CGCKK4PWRPE|           15|\n",
      "|    P36C37R961F|            4|\n",
      "|    PACRYZSQZBB|           37|\n",
      "|    E8Q18HZ1HA5|           18|\n",
      "|    M0AAYY64AE2|            7|\n",
      "|    CKAXDN66Q8Q|            4|\n",
      "|    M80ZDYV1W12|           61|\n",
      "|    TB73XFT6CS1|           39|\n",
      "|    0JRRSR1H8AA|            1|\n",
      "|    44829G8FBMV|           16|\n",
      "|    P0WN74JVXXB|           14|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Title/Season ID').agg({\"Row ID\":'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check parameter value we can modify at runtime or not.\n",
    "spark.conf.isModifiable(\"spark.executor.memory\")\n",
    "spark.conf.isModifiable(\"spark.sql.shuffle.partitions\")\n",
    "\n",
    "#set conf\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 3)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to get data count under each partition\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "def getPartitionCount(table):\n",
    "    ttl_num_partitions = table.withColumn(\"partitionid\",spark_partition_id()).select(\"partitionid\").groupBy(\"partitionid\").agg({'partitionid': \"count\"})\n",
    "    ttl_num_partitions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|partitionid|count(partitionid)|\n",
      "+-----------+------------------+\n",
      "|          0|              3981|\n",
      "|          1|              3981|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current partition count\n",
    "df.rdd.getNumPartitions()\n",
    "\n",
    "#get repartitioned count\n",
    "data = df.repartition(2)\n",
    "data.rdd.getNumPartitions()\n",
    "\n",
    "getPartitionCount(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDDs with same paritition count, key and type will produce same partitions\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"Sales2\",30), (\"Sales3\",30), (\"Sales4\",30), (\"IT\",40), (\"IT2\",40), (\"IT3\",40)]\n",
    "a_rdd = sc.parallelize(dept)\n",
    "#a.getNumPartitions()\n",
    "a_df = a_rdd.toDF()\n",
    "#a_df.show()\n",
    "a_df.withColumn(\"partitionid\",spark_partition_id()).show()\n",
    "\n",
    "ra_df = a_df.repartition(\"_2\")\n",
    "ra_df.withColumn(\"partitionid\",spark_partition_id()).show()\n",
    "\n",
    "\n",
    "\n",
    "a2_rdd = sc.parallelize(dept)\n",
    "a_df2 = a2_rdd.toDF()\n",
    "\n",
    "radf2 = a_df2.repartition(\"_2\")\n",
    "radf2.withColumn(\"partitionid\",spark_partition_id()).show()\n",
    "\n",
    "\n",
    "#compare both the table partitions\n",
    "getPartitionCount(ra_df)\n",
    "getPartitionCount(radf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+\n",
      "|       _1| _2|partitionid|\n",
      "+---------+---+-----------+\n",
      "|    Sales| 30|          0|\n",
      "|   Sales2| 30|          0|\n",
      "|   Sales3| 30|          0|\n",
      "|   Sales4| 30|          0|\n",
      "|Marketing| 20|          1|\n",
      "|       IT| 40|          1|\n",
      "|      IT2| 40|          1|\n",
      "|      IT3| 40|          1|\n",
      "|  Finance| 10|          2|\n",
      "+---------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ra_df.withColumn(\"partitionid\",spark_partition_id()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|    Sales| 30|\n",
      "|   Sales2| 30|\n",
      "|   Sales3| 30|\n",
      "|   Sales4| 30|\n",
      "|Marketing| 20|\n",
      "|       IT| 40|\n",
      "|      IT2| 40|\n",
      "|      IT3| 40|\n",
      "|  Finance| 10|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ra_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
