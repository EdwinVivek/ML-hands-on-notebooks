{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9343c344914e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[3]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"partitioning&shuffle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.jars.packages\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"com.crealytics:spark-excel_2.11:0.12.2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local[3]\").setAppName(\"partitioning&shuffle\").set(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.11:0.12.2\")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    ".option(\"useHeader\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"C:\\\\Users\\\\EdwinVivekN\\\\Desktop\\\\sample files\\\\test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".csv(\"C:\\\\Users\\\\EdwinVivekN\\Desktop\\\\sample files\\\\NEW_All_AppleTV_PrimeVideo_RevenueTrackin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: string (nullable = true)\n",
      " |-- Title/Season ID: string (nullable = true)\n",
      " |-- ASIN: string (nullable = true)\n",
      " |-- Title Name: string (nullable = true)\n",
      " |-- Series Name: string (nullable = true)\n",
      " |-- Season Name: string (nullable = true)\n",
      " |-- Consumption Type: string (nullable = true)\n",
      " |-- Quality: string (nullable = true)\n",
      " |-- Unit Price: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Duration Streamed: string (nullable = true)\n",
      " |-- Paid Ad Impressions: string (nullable = true)\n",
      " |-- Ad Revenue: string (nullable = true)\n",
      " |-- Royalty Rate: double (nullable = true)\n",
      " |-- Royalty Amount: string (nullable = true)\n",
      " |-- Royalty Currency: string (nullable = true)\n",
      " |-- Exchange rate: string (nullable = true)\n",
      " |-- $USD Royalty Amount: string (nullable = true)\n",
      " |--  Estimated $USD GROSS revenues : string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Territory: string (nullable = true)\n",
      " |-- Period Start: string (nullable = true)\n",
      " |-- Period End: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- CER Percentile: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7962"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        Unit Price|\n",
      "+-------+------------------+\n",
      "|  count|              4515|\n",
      "|   mean|5.5489700996681455|\n",
      "| stddev| 3.577174063761296|\n",
      "|    min|               0.0|\n",
      "|    max|             15.99|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.count()\n",
    "df.describe('Unit Price').show()\n",
    "df.select('Quality').distinct().count()\n",
    "\n",
    "#change data type\n",
    "changedTypedf = df.withColumn(\"Row ID\", df[\"Row ID\"].cast(DoubleType()))\n",
    "changedTypedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|         Colunm Name|Distinct Count|\n",
      "+--------------------+--------------+\n",
      "|              Row ID|          7962|\n",
      "|     Title/Season ID|           266|\n",
      "|                ASIN|          1104|\n",
      "|          Title Name|           255|\n",
      "|         Series Name|             5|\n",
      "|         Season Name|             5|\n",
      "|    Consumption Type|             3|\n",
      "|             Quality|             3|\n",
      "|          Unit Price|            88|\n",
      "|            Quantity|           326|\n",
      "|   Duration Streamed|          2793|\n",
      "| Paid Ad Impressions|             1|\n",
      "|          Ad Revenue|             1|\n",
      "|        Royalty Rate|             4|\n",
      "|      Royalty Amount|          1719|\n",
      "|    Royalty Currency|             4|\n",
      "|       Exchange rate|            25|\n",
      "| $USD Royalty Amount|          2574|\n",
      "| Estimated $USD G...|          2369|\n",
      "|              Region|             5|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType,StructField, StructType\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    rows.append([c, df.select(c).distinct().count()])\n",
    "df2 = spark.createDataFrame(rows, [\"Colunm Name\", \"Distinct Count\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|Title/Season ID|count(Row ID)|\n",
      "+---------------+-------------+\n",
      "|    8Z1Q3WNG809|          152|\n",
      "|    T88GM85KJ6M|           30|\n",
      "|    8QCXQQ7S13K|           28|\n",
      "|    RB0662ZDVHD|           14|\n",
      "|    GQ08HR5MC5H|            3|\n",
      "|    ESSAG5226FC|            4|\n",
      "|    63KGWNZ6PCV|            3|\n",
      "|    T6DMTK8PGZJ|            7|\n",
      "|    T7HWTHBJD56|           28|\n",
      "|    CGCKK4PWRPE|           15|\n",
      "|    P36C37R961F|            4|\n",
      "|    PACRYZSQZBB|           37|\n",
      "|    E8Q18HZ1HA5|           18|\n",
      "|    M0AAYY64AE2|            7|\n",
      "|    CKAXDN66Q8Q|            4|\n",
      "|    M80ZDYV1W12|           61|\n",
      "|    TB73XFT6CS1|           39|\n",
      "|    0JRRSR1H8AA|            1|\n",
      "|    44829G8FBMV|           16|\n",
      "|    P0WN74JVXXB|           14|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Title/Season ID').agg({\"Row ID\":'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check parameter value we can modify at runtime or not.\n",
    "spark.conf.isModifiable(\"spark.executor.memory\")\n",
    "spark.conf.isModifiable(\"spark.sql.shuffle.partitions\")\n",
    "\n",
    "#set conf\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get partitioned data count\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "def getPartitionCount(table):\n",
    "    ttl_num_partitions = table.withColumn(\"partitionid\",spark_partition_id()).select(\"partitionid\").groupBy(\"partitionid\").agg({'partitionid': \"count\"})\n",
    "    ttl_num_partitions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|partitionid|count(partitionid)|\n",
      "+-----------+------------------+\n",
      "|          0|              3981|\n",
      "|          1|              3981|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.repartition(2)\n",
    "data.rdd.getNumPartitions()\n",
    "\n",
    "getPartitionCount(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+\n",
      "|       _1| _2|partitionid|\n",
      "+---------+---+-----------+\n",
      "|  Finance| 10|          0|\n",
      "|Marketing| 20|          0|\n",
      "|    Sales| 30|          0|\n",
      "|   Sales2| 30|          1|\n",
      "|   Sales3| 30|          1|\n",
      "|   Sales4| 30|          1|\n",
      "|       IT| 40|          2|\n",
      "|      IT2| 40|          2|\n",
      "|      IT3| 40|          2|\n",
      "+---------+---+-----------+\n",
      "\n",
      "+-----------+------------------+\n",
      "|partitionid|count(partitionid)|\n",
      "+-----------+------------------+\n",
      "|          0|                 4|\n",
      "|          2|                 1|\n",
      "|          1|                 4|\n",
      "+-----------+------------------+\n",
      "\n",
      "+---------+---+-----------+\n",
      "|       _1| _2|partitionid|\n",
      "+---------+---+-----------+\n",
      "|    Sales| 30|          0|\n",
      "|   Sales2| 30|          0|\n",
      "|   Sales3| 30|          0|\n",
      "|   Sales4| 30|          0|\n",
      "|Marketing| 20|          1|\n",
      "|       IT| 40|          1|\n",
      "|      IT2| 40|          1|\n",
      "|      IT3| 40|          1|\n",
      "|  Finance| 10|          2|\n",
      "+---------+---+-----------+\n",
      "\n",
      "+-----------+------------------+\n",
      "|partitionid|count(partitionid)|\n",
      "+-----------+------------------+\n",
      "|          0|                 4|\n",
      "|          2|                 1|\n",
      "|          1|                 4|\n",
      "+-----------+------------------+\n",
      "\n",
      "+---------+---+-----------+\n",
      "|       _1| _2|partitionid|\n",
      "+---------+---+-----------+\n",
      "|    Sales| 30|          0|\n",
      "|   Sales2| 30|          0|\n",
      "|   Sales3| 30|          0|\n",
      "|   Sales4| 30|          0|\n",
      "|Marketing| 20|          1|\n",
      "|       IT| 40|          1|\n",
      "|      IT2| 40|          1|\n",
      "|      IT3| 40|          1|\n",
      "|  Finance| 10|          2|\n",
      "+---------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RDDs with same paritition count, key and type will produce same partitions\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"Sales2\",30), (\"Sales3\",30), (\"Sales4\",30), (\"IT\",40), (\"IT2\",40), (\"IT3\",40)]\n",
    "a = sc.parallelize(dept)\n",
    "#a.getNumPartitions()\n",
    "adf = a.toDF()\n",
    "#adf.show()\n",
    "adf.withColumn(\"partitionid\",spark_partition_id()).show()\n",
    "radf = adf.repartition(\"_2\")\n",
    "getPartitionCount(radf)\n",
    "radf.withColumn(\"partitionid\",spark_partition_id()).show()\n",
    "\n",
    "\n",
    "\n",
    "a2 = sc.parallelize(dept)\n",
    "adf2 = a2.toDF()\n",
    "radf2 = adf2.repartition(\"_2\")\n",
    "getPartitionCount(radf2)\n",
    "radf2.withColumn(\"partitionid\",spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
