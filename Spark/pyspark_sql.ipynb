{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://synclapn17855:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sample app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x9e43c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().set(\"spark.executor.memory\", \"1g\") \\\n",
    "    .set(\"spark.driver.memory\", \"4g\") \\\n",
    "    .setMaster(\"local[*]\").setAppName(\"sample app\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.getActiveSession()\n",
    "\n",
    "spark2 = spark.newSession()   # has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache.\n",
    "spark2\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext, HiveContext\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "#hive_context = HiveContext(sc)\n",
    "#or\n",
    "#sc = SparkContext(conf=conf)\n",
    "#spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hive session\n",
    "hive_session = SparkSession \\\n",
    "    .builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hello='spark')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql('''select 'spark' as hello''')\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- createdAt: date (nullable = true)\n",
      " |-- mapfield: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n",
      "+-------------------+------------------+-----+------+------+----------+-----------+\n",
      "|name               |languages         |state|gender|salary|createdAt |mapfield   |\n",
      "+-------------------+------------------+-----+------+------+----------+-----------+\n",
      "|[James, , Smith]   |[Java, Scala, C++]|OH   |M     |20000 |2019-12-01|[Jan -> 22]|\n",
      "|[Anna, Rose, ]     |[Spark, Java, C++]|NY   |F     |15000 |2019-12-14|[Feb -> 19]|\n",
      "|[Julia, , Williams]|[CSharp, VB]      |OH   |F     |10000 |2019-12-16|[Mar -> 30]|\n",
      "+-------------------+------------------+-----+------+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Create Dataframe programatically\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType, DateType, MapType\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\", 20000, datetime(2019, 12, 1), {\"Jan\": 22}),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\", 15000, datetime(2019, 12, 14), {\"Feb\": 19}),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\", 10000, datetime(2019, 12, 16), {\"Mar\": 30}),\n",
    " ]    \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True),\n",
    "     StructField('salary', IntegerType(), True),\n",
    "     StructField('createdAt', DateType(), True),\n",
    "     StructField('mapfield', MapType(StringType(), IntegerType()), True)\n",
    " ])\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+------+------+----------+------------+\n",
      "|                name|           languages|state|gender|salary| createdAt|    mapfield|\n",
      "+--------------------+--------------------+-----+------+------+----------+------------+\n",
      "|     [Edwin, N, Max]|[CSharp, Python, JS]|   CA|     M| 20000|2022-12-02|[April -> 4]|\n",
      "|[Jordan, E, Raphael]|                  []|   NY|     M| 50000|2023-05-24| [May -> 14]|\n",
      "+--------------------+--------------------+-----+------+------+----------+------------+\n",
      "\n",
      "+--------------------+--------------------+-----+------+------+----------+------------+\n",
      "|                name|           languages|state|gender|salary| createdAt|    mapfield|\n",
      "+--------------------+--------------------+-----+------+------+----------+------------+\n",
      "|    [James, , Smith]|  [Java, Scala, C++]|   OH|     M| 20000|2019-12-01| [Jan -> 22]|\n",
      "|      [Anna, Rose, ]|  [Spark, Java, C++]|   NY|     F| 15000|2019-12-14| [Feb -> 19]|\n",
      "| [Julia, , Williams]|        [CSharp, VB]|   OH|     F| 10000|2019-12-16| [Mar -> 30]|\n",
      "|     [Edwin, N, Max]|[CSharp, Python, JS]|   CA|     M| 20000|2022-12-02|[April -> 4]|\n",
      "|[Jordan, E, Raphael]|                  []|   NY|     M| 50000|2023-05-24| [May -> 14]|\n",
      "+--------------------+--------------------+-----+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#insert into dataframe\n",
    "data = [\n",
    "    ((\"Edwin\",\"N\",\"Max\"),[\"CSharp\",\"Python\",\"JS\"],\"CA\",\"M\", 20000, datetime(2022, 12, 2), {\"April\": 4}),\n",
    "    ((\"Jordan\", \"E\", \"Raphael\"), [], \"NY\", \"M\", 50000, datetime(2023,5,24), {\"May\": 14})\n",
    "]\n",
    "df2 = spark.createDataFrame(data, schema)\n",
    "df2.show()\n",
    "df2 = df.union(df2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name=Row(firstname='James', middlename='', lastname='Smith'), languages=['Java', 'Scala', 'C++'], state='OH', gender='M', salary=20000, createdAt=datetime.date(2019, 12, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['employee', 'employees']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/EdwinVivekN/AppData/Local/Programs/Python/Python37-32/myscripts/Spark/spark-warehouse')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='employee', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='employees', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registering tables and views\n",
    "df.registerTempTable(\"employee\")\n",
    "#or\n",
    "sqlContext.registerDataFrameAsTable(df, \"employee\")\n",
    "\n",
    "spark.sql(\"select * from employee\").head()\n",
    "\n",
    "\n",
    "#Registering views\n",
    "df.createOrReplaceGlobalTempView(\"employees_global\") #share data among different sessions and keep alive until your application ends\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "\n",
    "#List available table and views\n",
    "sqlContext.tableNames()\n",
    "#or\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "\n",
    "\n",
    "#Drop tables and views\n",
    "sqlContext.dropTempTable(\"employee\")\n",
    "spark.catalog.dropTempView(\"employees\")\n",
    "spark.catalog.dropGlobalTempView(\"employees_global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|firstname|languages[2]|\n",
      "+---------+------------+\n",
      "|    James|         C++|\n",
      "|     Anna|         C++|\n",
      "|    Julia|        null|\n",
      "+---------+------------+\n",
      "\n",
      "+------------+-----------+\n",
      "|languages[2]|sum(salary)|\n",
      "+------------+-----------+\n",
      "|        null|      10000|\n",
      "|         C++|      35000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accessing nested elements\n",
    "df.createOrReplaceTempView(\"employee\")\n",
    "spark.sql('''select name.firstname, languages[2] from employee''').show()\n",
    "\n",
    "spark.sql('''select employee.languages[2], sum(salary) from employee group by employee.languages[2]''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with Row object\n",
    "from pyspark.sql import Row\n",
    "Person = Row(\"name\", \"age\")\n",
    "Person\n",
    "rdd = sc.parallelize([(\"alice\", 22)])\n",
    "person = rdd.map(lambda r: Person(*r))\n",
    "person.collect()\n",
    "\n",
    "for p in person.collect():\n",
    "    print(p.name + \" \" + str(p.age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name=Row(firstname='James', middlename='', lastname='Smith'), languages=['Java', 'Scala', 'C++'], state='OH', gender='M', salary=20000, createdAt=datetime.date(2019, 12, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Java', 'Scala', 'C++']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Row(firstname='James', middlename='', lastname='Smith')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'name': {'firstname': 'James', 'middlename': '', 'lastname': 'Smith'},\n",
       " 'languages': ['Java', 'Scala', 'C++'],\n",
       " 'state': 'OH',\n",
       " 'gender': 'M',\n",
       " 'salary': 20000,\n",
       " 'createdAt': datetime.date(2019, 12, 1)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = spark.sql(\"select * from employee\").head()\n",
    "row\n",
    "row.languages\n",
    "row[\"name\"]\n",
    "row.asDict(True) #recursive - true, turns the nested Row as dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caching \n",
    "sqlContext.registerDataFrameAsTable(df, \"employee\")\n",
    "sqlContext.cacheTable(\"employee\")\n",
    "spark.sql(\"select * from employee\").head()\n",
    "sqlContext.uncacheTable(\"employee\")\n",
    "\n",
    "sqlContext.clearCache() # clear entire cache\n",
    "\n",
    "spark.catalog.refreshTable(\"employee\") # refresh all the cached metadata\n",
    "\n",
    "#persist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "#JSON\n",
    "dataframe = spark.read.json('json_data.json')\n",
    "#TXT FILES# \n",
    "dataframe_txt = spark.read.text('text_data.txt')\n",
    "#PARQUET FILES# \n",
    "dataframe_parquet = spark.read.load('parquet_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Loan_ID', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('Married', 'string'),\n",
       " ('Dependents', 'string'),\n",
       " ('Education', 'string'),\n",
       " ('Self_Employed', 'string'),\n",
       " ('ApplicantIncome', 'int'),\n",
       " ('CoapplicantIncome', 'int'),\n",
       " ('LoanAmount', 'int'),\n",
       " ('Loan_Amount_Term', 'int'),\n",
       " ('Credit_History', 'int'),\n",
       " ('Property_Area', 'string'),\n",
       " ('Loan_Status', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------+------------------+------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\n",
      "|summary| Loan_ID|Gender|Married|        Dependents|   Education|Self_Employed|  ApplicantIncome|CoapplicantIncome|        LoanAmount| Loan_Amount_Term|    Credit_History|Property_Area|Loan_Status|\n",
      "+-------+--------+------+-------+------------------+------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\n",
      "|  count|     614|   601|    611|               599|         614|          582|              614|              614|               592|              600|               564|          614|        614|\n",
      "|   mean|    null|  null|   null|0.5547445255474452|        null|         null|5403.459283387622|1621.244299674267|146.41216216216216|            342.0|0.8421985815602837|         null|       null|\n",
      "| stddev|    null|  null|   null|0.7853289861674311|        null|         null|6109.041673387181|2926.248760179483| 85.58732523570545|65.12040985461255|0.3648783192364052|         null|       null|\n",
      "|    min|LP001002|Female|     No|                 0|    Graduate|           No|              150|                0|                 9|               12|                 0|        Rural|          N|\n",
      "|    max|LP002990|  Male|    Yes|                3+|Not Graduate|          Yes|            81000|            41667|               700|              480|                 1|        Urban|          Y|\n",
      "+-------+--------+------+-------+------------------+------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Loan_ID',\n",
       " 'Gender',\n",
       " 'Married',\n",
       " 'Dependents',\n",
       " 'Education',\n",
       " 'Self_Employed',\n",
       " 'ApplicantIncome',\n",
       " 'CoapplicantIncome',\n",
       " 'LoanAmount',\n",
       " 'Loan_Amount_Term',\n",
       " 'Credit_History',\n",
       " 'Property_Area',\n",
       " 'Loan_Status']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect data\n",
    "loan = spark.read.csv('..\\data\\\\train_loanpred.csv', inferSchema=True, header=True)\n",
    "loan.dtypes\n",
    "loan.columns\n",
    "loan.count()\n",
    "loan.distinct().count()\n",
    "loan.printSchema()\n",
    "loan.describe().show()\n",
    "df.describe('ApplicantIncome').show()\n",
    "df.select('Gender').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation[Loan_ID#2416,Gender#2417,Married#2418,Dependents#2419,Education#2420,Self_Employed#2421,ApplicantIncome#2422,CoapplicantIncome#2423,LoanAmount#2424,Loan_Amount_Term#2425,Credit_History#2426,Property_Area#2427,Loan_Status#2428] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Loan_ID: string, Gender: string, Married: string, Dependents: string, Education: string, Self_Employed: string, ApplicantIncome: int, CoapplicantIncome: int, LoanAmount: int, Loan_Amount_Term: int, Credit_History: int, Property_Area: string, Loan_Status: string\n",
      "Relation[Loan_ID#2416,Gender#2417,Married#2418,Dependents#2419,Education#2420,Self_Employed#2421,ApplicantIncome#2422,CoapplicantIncome#2423,LoanAmount#2424,Loan_Amount_Term#2425,Credit_History#2426,Property_Area#2427,Loan_Status#2428] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation[Loan_ID#2416,Gender#2417,Married#2418,Dependents#2419,Education#2420,Self_Employed#2421,ApplicantIncome#2422,CoapplicantIncome#2423,LoanAmount#2424,Loan_Amount_Term#2425,Credit_History#2426,Property_Area#2427,Loan_Status#2428] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [Loan_ID#2416,Gender#2417,Married#2418,Dependents#2419,Education#2420,Self_Employed#2421,ApplicantIncome#2422,CoapplicantIncome#2423,LoanAmount#2424,Loan_Amount_Term#2425,Credit_History#2426,Property_Area#2427,Loan_Status#2428] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/EdwinVivekN/AppData/Local/Programs/Python/Python37-32/myscripts/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Loan_ID:string,Gender:string,Married:string,Dependents:string,Education:string,Self_Employ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+----------+------------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "| Loan_ID|Gender|Married|Dependents|   Education|Self_Employed|ApplicantIncome|CoapplicantIncome|LoanAmount|Loan_Amount_Term|Credit_History|Property_Area|Loan_Status|\n",
      "+--------+------+-------+----------+------------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "|LP001002|  Male|     No|         0|    Graduate|           No|           5849|                0|      null|             360|             1|        Urban|          Y|\n",
      "|LP001003|  Male|    Yes|         1|    Graduate|           No|           4583|             1508|       128|             360|             1|        Rural|          N|\n",
      "|LP001005|  Male|    Yes|         0|    Graduate|          Yes|           3000|                0|        66|             360|             1|        Urban|          Y|\n",
      "|LP001006|  Male|    Yes|         0|Not Graduate|           No|           2583|             2358|       120|             360|             1|        Urban|          Y|\n",
      "|LP001008|  Male|     No|         0|    Graduate|           No|           6000|                0|       141|             360|             1|        Urban|          Y|\n",
      "+--------+------+-------+----------+------------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Loan_ID='LP001002', Gender='Male', Married='No', Dependents='0', Education='Graduate', Self_Employed='No', ApplicantIncome=5849, CoapplicantIncome=0, LoanAmount=None, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y'),\n",
       " Row(Loan_ID='LP001003', Gender='Male', Married='Yes', Dependents='1', Education='Graduate', Self_Employed='No', ApplicantIncome=4583, CoapplicantIncome=1508, LoanAmount=128, Loan_Amount_Term=360, Credit_History=1, Property_Area='Rural', Loan_Status='N'),\n",
       " Row(Loan_ID='LP001005', Gender='Male', Married='Yes', Dependents='0', Education='Graduate', Self_Employed='Yes', ApplicantIncome=3000, CoapplicantIncome=0, LoanAmount=66, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y'),\n",
       " Row(Loan_ID='LP001006', Gender='Male', Married='Yes', Dependents='0', Education='Not Graduate', Self_Employed='No', ApplicantIncome=2583, CoapplicantIncome=2358, LoanAmount=120, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y'),\n",
       " Row(Loan_ID='LP001008', Gender='Male', Married='No', Dependents='0', Education='Graduate', Self_Employed='No', ApplicantIncome=6000, CoapplicantIncome=0, LoanAmount=141, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Row(Loan_ID='LP001002', Gender='Male', Married='No', Dependents='0', Education='Graduate', Self_Employed='No', ApplicantIncome=5849, CoapplicantIncome=0, LoanAmount=None, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Row(Loan_ID='LP001002', Gender='Male', Married='No', Dependents='0', Education='Graduate', Self_Employed='No', ApplicantIncome=5849, CoapplicantIncome=0, LoanAmount=None, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan.show(5)\n",
    "loan.take(5)\n",
    "loan.head()\n",
    "loan.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "|Loan_ID|Gender|Married|Dependents|Education|Self_Employed|ApplicantIncome|CoapplicantIncome|LoanAmount|Loan_Amount_Term|Credit_History|Property_Area|Loan_Status|\n",
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "|      0|    13|      3|        15|        0|           32|              0|                0|        22|              14|            50|            0|          0|\n",
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "\n",
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "|Loan_ID|Gender|Married|Dependents|Education|Self_Employed|ApplicantIncome|CoapplicantIncome|LoanAmount|Loan_Amount_Term|Credit_History|Property_Area|Loan_Status|\n",
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "|      0|    13|      3|        15|        0|           32|              0|                0|        22|              14|            50|            0|          0|\n",
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dependents',\n",
       " 'Loan_Amount_Term',\n",
       " 'Self_Employed',\n",
       " 'Credit_History',\n",
       " 'LoanAmount',\n",
       " 'Gender',\n",
       " 'Married']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+--------------+----------+------+-------+\n",
      "|Dependents|Loan_Amount_Term|Self_Employed|Credit_History|LoanAmount|Gender|Married|\n",
      "+----------+----------------+-------------+--------------+----------+------+-------+\n",
      "|         0|               0|            0|             0|         0|     0|      0|\n",
      "+----------+----------------+-------------+--------------+----------+------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Gender|\n",
      "+------+\n",
      "|     T|\n",
      "|Female|\n",
      "|  Male|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|Gender|\n",
      "+------+\n",
      "|Female|\n",
      "| Child|\n",
      "|  Male|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find Count of Null, None, NaN of All DataFrame Columns\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "loan.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in loan.columns]).show()\n",
    "\n",
    "loan.select([count(when(loan[c].isNull() == True, 1)).alias(c) for c in loan.columns]).show()\n",
    "\n",
    "# To list only columns with NANs\n",
    "import pandas as pd\n",
    "loan_collected = loan.collect()\n",
    "nan_columns = [''.join(key) for _ in loan_collected  for (key,val) in _.asDict().items() if pd.isna(val)]\n",
    "nan_columns = list(set(nan_columns)) \n",
    "nan_columns\n",
    "loan.select([count(when((isnan(c)),c)).alias(c) for c in nan_columns]).show()\n",
    "\n",
    "\n",
    "#drop duplicate\n",
    "uLoan = loan.dropDuplicates(['Gender'])\n",
    "uLoan.count()\n",
    "\n",
    "#drop NA\n",
    "nLoan = loan.dropna(how='any')\n",
    "nLoan.count()\n",
    "\n",
    "#fill NA\n",
    "fLoan = loan.fillna({\"Gender\": \"T\"})\n",
    "fLoan.select(\"Gender\").distinct().show()\n",
    "\n",
    "#replace data\n",
    "rLoan = fLoan.na.replace(\"T\", \"Child\", \"Gender\") #old, new, columnname\n",
    "rLoan.select(\"Gender\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Loan_ID='LP001002', Gender='Male', Married='No', Dependents='0', Education='Graduate', Self_Employed='No', ApplicantIncome=5849, CoapplicantIncome=0, LoanAmount=None, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y', ApplicantIncome_New=15849)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Row(Loan_ID='LP001002', Gender='Male', Married='No', Dependents='0', Education='Graduate', Self_Employed='No', ApplicantIncome=5849, CoapplicantIncome=0, LoanAmount=None, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y', NewApplicantIncome=15849)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Loan_ID',\n",
       " 'Gender',\n",
       " 'Married',\n",
       " 'Dependents',\n",
       " 'Education',\n",
       " 'Self_Employed',\n",
       " 'ApplicantIncome',\n",
       " 'CoapplicantIncome',\n",
       " 'LoanAmount',\n",
       " 'Loan_Amount_Term',\n",
       " 'Credit_History',\n",
       " 'Property_Area',\n",
       " 'Loan_Status']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add/remove/update columns\n",
    "\n",
    "#add\n",
    "new_loan = loan.withColumn(\"ApplicantIncome_New\", loan.ApplicantIncome + 10000)\n",
    "new_loan.first()\n",
    "\n",
    "#update\n",
    "new_loan = new_loan.withColumnRenamed(\"ApplicantIncome_New\", \"NewApplicantIncome\")\n",
    "new_loan.head()\n",
    "\n",
    "#remove\n",
    "new_loan.drop(\"NewApplicantIncome\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repartition\n",
    "# Dataframe with 10 partitions\n",
    "loan.repartition(10).rdd.getNumPartitions()\n",
    "\n",
    "# Dataframe with 1 partition\n",
    "loan.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframe into an RDD\n",
    "df_to_rdd = df.rdd\n",
    "df_to_rdd.map(lambda x: x.hello + \"application\").collect()\n",
    "\n",
    "#Converting RDD into a dataframe\n",
    "rdd_to_df = df_to_rdd.toDF()\n",
    "rdd_to_df.show()\n",
    "\n",
    "# Converting dataframe into a RDD of string \n",
    "df.toJSON().first()\n",
    "\n",
    "# Obtaining contents of df as Pandas \n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write and Save\n",
    "df.select(\"firstName\", \"city\").write.save(\"nameAndCity.parquet\")\n",
    "\n",
    "df.select(\"firstName\", \"age\").write.save(\"namesAndAges.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+------+------+----------+\n",
      "|               name|         languages|state|gender|salary| createdAt|\n",
      "+-------------------+------------------+-----+------+------+----------+\n",
      "|   [James, , Smith]|[Java, Scala, C++]|   OH|     M| 20000|2019-12-01|\n",
      "|     [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "|[Julia, , Williams]|      [CSharp, VB]|   OH|     F| 10000|2019-12-16|\n",
      "+-------------------+------------------+-----+------+------+----------+\n",
      "\n",
      "+-------------------+------------------+\n",
      "|               name|         languages|\n",
      "+-------------------+------------------+\n",
      "|   [James, , Smith]|[Java, Scala, C++]|\n",
      "|     [Anna, Rose, ]|[Spark, Java, C++]|\n",
      "|[Julia, , Williams]|      [CSharp, VB]|\n",
      "+-------------------+------------------+\n",
      "\n",
      "+--------------+------+------------+\n",
      "|name.firstname|gender|lang_flatten|\n",
      "+--------------+------+------------+\n",
      "|         James|     M|        Java|\n",
      "|         James|     M|       Scala|\n",
      "|         James|     M|         C++|\n",
      "|          Anna|     F|       Spark|\n",
      "|          Anna|     F|        Java|\n",
      "|          Anna|     F|         C++|\n",
      "|         Julia|     F|      CSharp|\n",
      "|         Julia|     F|          VB|\n",
      "+--------------+------+------------+\n",
      "\n",
      "+---------+---------+----------+\n",
      "|firstname|lang_flat|lang_flat2|\n",
      "+---------+---------+----------+\n",
      "|    James|     Java|      Java|\n",
      "|    James|     Java|     Scala|\n",
      "|    James|     Java|       C++|\n",
      "|    James|    Scala|      Java|\n",
      "|    James|    Scala|     Scala|\n",
      "|    James|    Scala|       C++|\n",
      "|    James|      C++|      Java|\n",
      "|    James|      C++|     Scala|\n",
      "|    James|      C++|       C++|\n",
      "|     Anna|    Spark|     Spark|\n",
      "|     Anna|    Spark|      Java|\n",
      "|     Anna|    Spark|       C++|\n",
      "|     Anna|     Java|     Spark|\n",
      "|     Anna|     Java|      Java|\n",
      "|     Anna|     Java|       C++|\n",
      "|     Anna|      C++|     Spark|\n",
      "|     Anna|      C++|      Java|\n",
      "|     Anna|      C++|       C++|\n",
      "|    Julia|   CSharp|    CSharp|\n",
      "|    Julia|   CSharp|        VB|\n",
      "+---------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------------+\n",
      "|salary|(salary > 15000)|\n",
      "+------+----------------+\n",
      "| 20000|            true|\n",
      "| 15000|           false|\n",
      "| 10000|           false|\n",
      "+------+----------------+\n",
      "\n",
      "+------+----------------+\n",
      "|salary|(salary + 15000)|\n",
      "+------+----------------+\n",
      "| 20000|           35000|\n",
      "| 15000|           30000|\n",
      "| 10000|           25000|\n",
      "+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select\n",
    "import pyspark.sql.functions as F\n",
    "df.select(\"*\").show()\n",
    "df.select(\"name\", \"languages\").show()\n",
    "\n",
    "#flatten single column\n",
    "df.select(\"name.firstname\", \"gender\", F.explode(\"languages\").alias(\"lang_flatten\")).show()\n",
    "\n",
    "#flatten multiple columns \n",
    "explodedDF = df.withColumn(\"lang_flat\", F.explode(\"languages\")).withColumn(\"lang_flat2\", F.explode(\"languages\"))\n",
    "explodedDF.select(\"name.firstname\", \"lang_flat\", \"lang_flat2\").show()\n",
    "\n",
    "df.select(\"salary\", df[\"salary\"] > 15000).show()\n",
    "\n",
    "df.select(\"salary\", df[\"salary\"] + 15000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-355c250f67d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#When\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"salary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"salary\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m15000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"salary condition\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#otherwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"salary\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m15000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"false!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#When\n",
    "df.select(\"salary\", F.when(df[\"salary\"] > 15000, \"true!\").alias(\"salary condition\")).alias(\"sal\").show()\n",
    "\n",
    "#otherwise\n",
    "df.select(F.when(df[\"salary\"] > 15000, \"true!\").otherwise(\"false!\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-----+------+------+----------+\n",
      "|          name|         languages|state|gender|salary| createdAt|\n",
      "+--------------+------------------+-----+------+------+----------+\n",
      "|[Anna, Rose, ]|[Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "+--------------+------------------+-----+------+------+----------+\n",
      "\n",
      "+-------------------------+\n",
      "|name[firstname] LIKE Anna|\n",
      "+-------------------------+\n",
      "|                    false|\n",
      "|                     true|\n",
      "|                    false|\n",
      "+-------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|CASE WHEN startswith(name[firstname], Anna) THEN name[firstname] ELSE Not Anna END|\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                                                          Not Anna|\n",
      "|                                                                              Anna|\n",
      "|                                                                          Not Anna|\n",
      "+----------------------------------------------------------------------------------+\n",
      "\n",
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| Jam|\n",
      "| Ann|\n",
      "| Jul|\n",
      "+----+\n",
      "\n",
      "+-----------------------------------------+\n",
      "|((salary >= 15000) AND (salary <= 20000))|\n",
      "+-----------------------------------------+\n",
      "|                                     true|\n",
      "|                                     true|\n",
      "|                                    false|\n",
      "+-----------------------------------------+\n",
      "\n",
      "+--------------+------------------+-----+------+------+----------+\n",
      "|          name|         languages|state|gender|salary| createdAt|\n",
      "+--------------+------------------+-----+------+------+----------+\n",
      "|[Anna, Rose, ]|[Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "+--------------+------------------+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Like\n",
    "df[df.name.firstname.like(\"Anna\")].show()\n",
    "df.select(df.name.firstname.like(\"Anna\")).show()\n",
    "\n",
    "#startswith\n",
    "df.select(when(df.name.firstname.startswith(\"Anna\"), df.name.firstname).otherwise(\"Not Anna\")).show()\n",
    "\n",
    "#substring\n",
    "df.select(df.name.firstName.substr(1, 3).alias(\"name\")).show()\n",
    "\n",
    "#between\n",
    "df.select(df.salary.between(15000, 20000)).show()\n",
    "\n",
    "#In\n",
    "df[df.name.firstname.isin(\"Anna\")].show()\n",
    "\n",
    "#count\n",
    "df.select(F.count(df.name.firstname)).show()\n",
    "\n",
    "#filter\n",
    "df[df.salary >= 20000].show()\n",
    "df.filter(df.salary >= 20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|count(name.firstname AS `firstname`)|\n",
      "+------------------------------------+\n",
      "|                                   1|\n",
      "|                                   1|\n",
      "+------------------------------------+\n",
      "\n",
      "+-----------------------+-----+\n",
      "|name[firstname] LIKE J%|count|\n",
      "+-----------------------+-----+\n",
      "|                   true|    2|\n",
      "|                  false|    1|\n",
      "+-----------------------+-----+\n",
      "\n",
      "+--------------------------------------------------------+\n",
      "|sum(CASE WHEN name[firstname] LIKE J% THEN 1 ELSE 0 END)|\n",
      "+--------------------------------------------------------+\n",
      "|                                                       2|\n",
      "+--------------------------------------------------------+\n",
      "\n",
      "+-----+----------+----------+\n",
      "|state|sum_salary|min_salary|\n",
      "+-----+----------+----------+\n",
      "|   OH|     30000|     10000|\n",
      "|   NY|     15000|     15000|\n",
      "+-----+----------+----------+\n",
      "\n",
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   OH|     30000|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group By\n",
    "spark.sql('''select count(name.firstname) from employee where name.firstname like 'J%' group by name.firstname''').show()\n",
    "\n",
    "df.groupBy(df.name.firstname.like('J%')).count().show()\n",
    "\n",
    "df.select(df.name.firstname, when(df.name.firstname.like(\"J%\"), 1).otherwise(0)).groupBy().sum().show()\n",
    "\n",
    "\n",
    "#multiple aggregates on group by\n",
    "df.groupBy(df.state).agg(F.sum(df.salary).alias(\"sum_salary\"), F.min(df.salary).alias(\"min_salary\")).show()\n",
    "\n",
    "\n",
    "##Having\n",
    "df.groupBy(df.state).agg(F.sum(df.salary).alias(\"sum_salary\")).where(col(\"sum_salary\") > 20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join\n",
    "\n",
    "#window - rows, range\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+------+------+----------+\n",
      "|               name|           languages|state|gender|salary| createdAt|\n",
      "+-------------------+--------------------+-----+------+------+----------+\n",
      "|   [James, , Smith]|  [Java, Scala, C++]|   OH|     M| 20000|2019-12-01|\n",
      "|     [Anna, Rose, ]|  [Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "|[Julia, , Williams]|        [CSharp, VB]|   OH|     F| 10000|2019-12-16|\n",
      "|    [Edwin, N, Max]|[CSharp, Python, JS]|   CA|     M| 20000|2022-12-02|\n",
      "+-------------------+--------------------+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"employees\")\n",
    "spark.sql('''select * from employees''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----+-------------------+--------------------+-----+------+------+----------+\n",
      "|rownum|dense_rank|rank|               name|           languages|state|gender|salary| createdAt|\n",
      "+------+----------+----+-------------------+--------------------+-----+------+------+----------+\n",
      "|     1|         1|   1|    [Edwin, N, Max]|[CSharp, Python, JS]|   CA|     M| 20000|2022-12-02|\n",
      "|     1|         1|   1|   [James, , Smith]|  [Java, Scala, C++]|   OH|     M| 20000|2019-12-01|\n",
      "|     2|         1|   2|[Julia, , Williams]|        [CSharp, VB]|   OH|     F| 10000|2019-12-16|\n",
      "|     1|         2|   1|     [Anna, Rose, ]|  [Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "+------+----------+----+-------------------+--------------------+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select \\\n",
    "Row_Number() over (partition by state order by salary DESC, state ASC) as rownum, \\\n",
    "Dense_Rank() over (partition by gender order by salary ASC) as dense_rank, \\\n",
    "Rank() over (partition by state order by salary DESC) as rank, \\\n",
    "* from employees''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-----------+-----+---------+-------------------+--------------------+-----+------+------+----------+\n",
      "|rownum|rank|dense_rank|percentrank|ntile|cume_dist|               name|           languages|state|gender|salary| createdAt|\n",
      "+------+----+----------+-----------+-----+---------+-------------------+--------------------+-----+------+------+----------+\n",
      "|     4|   1|         1|        0.0|    1|      0.5|[Julia, , Williams]|        [CSharp, VB]|   OH|     F| 10000|2019-12-16|\n",
      "|     3|   1|         2|        0.0|    1|      1.0|     [Anna, Rose, ]|  [Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "|     1|   1|         1|        0.0|    1|      1.0|    [Edwin, N, Max]|[CSharp, Python, JS]|   CA|     M| 20000|2022-12-02|\n",
      "|     2|   2|         1|        1.0|    1|      1.0|   [James, , Smith]|  [Java, Scala, C++]|   OH|     M| 20000|2019-12-01|\n",
      "+------+----+----------+-----------+-----+---------+-------------------+--------------------+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select \\\n",
    "Row_Number() over (order by salary DESC, state ASC) as rownum, \\\n",
    "Rank() over (partition by state order by salary ASC) as rank, \\\n",
    "Dense_Rank() over (partition by gender order by salary ASC) as dense_rank, \\\n",
    "Percent_Rank() over (partition by state order by salary ASC) as percentrank, \\\n",
    "Ntile() over (partition by state order by salary ASC) as ntile, \\\n",
    "Cume_dist() over (partition by state order by salary ASC) as cume_dist, \\\n",
    "* from employees''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+---------+-----+------+\n",
      "|first_value|last_RANGE|last_ROWS|last_rows|state|gender|\n",
      "+-----------+----------+---------+---------+-----+------+\n",
      "|          M|         M|        M|        M|   CA|     M|\n",
      "|          M|         M|        F|        F|   OH|     M|\n",
      "|          M|         F|        F|        F|   OH|     F|\n",
      "|          F|         F|        F|        F|   NY|     F|\n",
      "+-----------+----------+---------+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select \\\n",
    "First_value(gender) over (partition by state order by gender DESC) as first_value, \\\n",
    "Last_value(gender) over (partition by state order by gender DESC) as last_RANGE, \\\n",
    "Last_value(gender) over (partition by state) as last_ROWS, \\\n",
    "Last_value(gender) over (partition by state order by gender DESC rows between UNBOUNDED PRECEDING and UNBOUNDED FOLLOWING) as last_rows, \\\n",
    "state,gender from employees''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------+-------------------+--------------------+-----+------+------+----------+\n",
      "|rolling_avg_salary|rolling_sum_salary|count_gender|               name|           languages|state|gender|salary| createdAt|\n",
      "+------------------+------------------+------------+-------------------+--------------------+-----+------+------+----------+\n",
      "|           20000.0|             20000|           1|    [Edwin, N, Max]|[CSharp, Python, JS]|   CA|     M| 20000|2022-12-02|\n",
      "|           10000.0|             10000|           1|[Julia, , Williams]|        [CSharp, VB]|   OH|     F| 10000|2019-12-16|\n",
      "|           15000.0|             30000|           2|   [James, , Smith]|  [Java, Scala, C++]|   OH|     M| 20000|2019-12-01|\n",
      "|           15000.0|             15000|           1|     [Anna, Rose, ]|  [Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "+------------------+------------------+------------+-------------------+--------------------+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select \\\n",
    "Avg(salary) over (partition by state order by gender ASC) as rolling_avg_salary, \\\n",
    "Sum(salary) over (partition by state order by gender ASC) as rolling_sum_salary, \\\n",
    "Count(gender) over (partition by state order by gender ASC) as count_gender, \\\n",
    "* from employees''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------------------+--------------------+-----+------+------+----------+\n",
      "|rolling_avg_salary|rolling_sum_salary|               name|           languages|state|gender|salary| createdAt|\n",
      "+------------------+------------------+-------------------+--------------------+-----+------+------+----------+\n",
      "|           20000.0|             20000|    [Edwin, N, Max]|[CSharp, Python, JS]|   CA|     M| 20000|2022-12-02|\n",
      "|           10000.0|             10000|[Julia, , Williams]|        [CSharp, VB]|   OH|     F| 10000|2019-12-16|\n",
      "|           15000.0|             30000|   [James, , Smith]|  [Java, Scala, C++]|   OH|     M| 20000|2019-12-01|\n",
      "|           15000.0|             15000|     [Anna, Rose, ]|  [Spark, Java, C++]|   NY|     F| 15000|2019-12-14|\n",
      "+------------------+------------------+-------------------+--------------------+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "window = Window.partitionBy(\"state\").orderBy(\"gender\")\n",
    "df2.select(F.avg(\"salary\").over(window).alias(\"rolling_avg_salary\"), \\\n",
    "           F.sum(\"salary\").over(window).alias(\"rolling_sum_salary\"), \\\n",
    "           \"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|array(gender, state)|\n",
      "+--------------------+\n",
      "|             [M, OH]|\n",
      "|             [F, NY]|\n",
      "|             [F, OH]|\n",
      "|             [M, CA]|\n",
      "|             [M, NY]|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+---------------------------+\n",
      "|           languages|sort_array(languages, true)|\n",
      "+--------------------+---------------------------+\n",
      "|  [Java, Scala, C++]|         [C++, Java, Scala]|\n",
      "|  [Spark, Java, C++]|         [C++, Java, Spark]|\n",
      "|        [CSharp, VB]|               [CSharp, VB]|\n",
      "|[CSharp, Python, JS]|       [CSharp, JS, Python]|\n",
      "|                  []|                         []|\n",
      "+--------------------+---------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(flatten(langoflang)=['Java', 'Scala', 'C++', 'Java', 'Scala', 'C++']),\n",
       " Row(flatten(langoflang)=['Spark', 'Java', 'C++', 'Spark', 'Java', 'C++']),\n",
       " Row(flatten(langoflang)=['CSharp', 'VB', 'CSharp', 'VB']),\n",
       " Row(flatten(langoflang)=['CSharp', 'Python', 'JS', 'CSharp', 'Python', 'JS']),\n",
       " Row(flatten(langoflang)=[])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Array Functions\n",
    "df2.select(F.array(\"gender\", \"state\")).show()\n",
    "\n",
    "df2.select(\"languages\", F.sort_array(\"languages\", True)).show()\n",
    "\n",
    "nestedArrayDF = df2.select(F.array(\"languages\", \"languages\").alias(\"langoflang\"))\n",
    "nestedArrayDF.select(F.flatten(\"langoflang\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+\n",
      "|firstname|           languages|lang_flat|\n",
      "+---------+--------------------+---------+\n",
      "|    James|  [Java, Scala, C++]|     Java|\n",
      "|    James|  [Java, Scala, C++]|    Scala|\n",
      "|    James|  [Java, Scala, C++]|      C++|\n",
      "|     Anna|  [Spark, Java, C++]|    Spark|\n",
      "|     Anna|  [Spark, Java, C++]|     Java|\n",
      "|     Anna|  [Spark, Java, C++]|      C++|\n",
      "|    Julia|        [CSharp, VB]|   CSharp|\n",
      "|    Julia|        [CSharp, VB]|       VB|\n",
      "|    Edwin|[CSharp, Python, JS]|   CSharp|\n",
      "|    Edwin|[CSharp, Python, JS]|   Python|\n",
      "|    Edwin|[CSharp, Python, JS]|       JS|\n",
      "+---------+--------------------+---------+\n",
      "\n",
      "+---------+--------------------+---------------+\n",
      "|firstname|           languages|lang_flat_outer|\n",
      "+---------+--------------------+---------------+\n",
      "|    James|  [Java, Scala, C++]|           Java|\n",
      "|    James|  [Java, Scala, C++]|          Scala|\n",
      "|    James|  [Java, Scala, C++]|            C++|\n",
      "|     Anna|  [Spark, Java, C++]|          Spark|\n",
      "|     Anna|  [Spark, Java, C++]|           Java|\n",
      "|     Anna|  [Spark, Java, C++]|            C++|\n",
      "|    Julia|        [CSharp, VB]|         CSharp|\n",
      "|    Julia|        [CSharp, VB]|             VB|\n",
      "|    Edwin|[CSharp, Python, JS]|         CSharp|\n",
      "|    Edwin|[CSharp, Python, JS]|         Python|\n",
      "|    Edwin|[CSharp, Python, JS]|             JS|\n",
      "|   Jordan|                  []|           null|\n",
      "+---------+--------------------+---------------+\n",
      "\n",
      "+--------------+--------------------+---+------+\n",
      "|name.firstname|           languages|pos|   col|\n",
      "+--------------+--------------------+---+------+\n",
      "|         James|  [Java, Scala, C++]|  0|  Java|\n",
      "|         James|  [Java, Scala, C++]|  1| Scala|\n",
      "|         James|  [Java, Scala, C++]|  2|   C++|\n",
      "|          Anna|  [Spark, Java, C++]|  0| Spark|\n",
      "|          Anna|  [Spark, Java, C++]|  1|  Java|\n",
      "|          Anna|  [Spark, Java, C++]|  2|   C++|\n",
      "|         Julia|        [CSharp, VB]|  0|CSharp|\n",
      "|         Julia|        [CSharp, VB]|  1|    VB|\n",
      "|         Edwin|[CSharp, Python, JS]|  0|CSharp|\n",
      "|         Edwin|[CSharp, Python, JS]|  1|Python|\n",
      "|         Edwin|[CSharp, Python, JS]|  2|    JS|\n",
      "+--------------+--------------------+---+------+\n",
      "\n",
      "+--------------+--------------------+----+------+\n",
      "|name.firstname|           languages| pos|   col|\n",
      "+--------------+--------------------+----+------+\n",
      "|         James|  [Java, Scala, C++]|   0|  Java|\n",
      "|         James|  [Java, Scala, C++]|   1| Scala|\n",
      "|         James|  [Java, Scala, C++]|   2|   C++|\n",
      "|          Anna|  [Spark, Java, C++]|   0| Spark|\n",
      "|          Anna|  [Spark, Java, C++]|   1|  Java|\n",
      "|          Anna|  [Spark, Java, C++]|   2|   C++|\n",
      "|         Julia|        [CSharp, VB]|   0|CSharp|\n",
      "|         Julia|        [CSharp, VB]|   1|    VB|\n",
      "|         Edwin|[CSharp, Python, JS]|   0|CSharp|\n",
      "|         Edwin|[CSharp, Python, JS]|   1|Python|\n",
      "|         Edwin|[CSharp, Python, JS]|   2|    JS|\n",
      "|        Jordan|                  []|null|  null|\n",
      "+--------------+--------------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generartor Functions\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#explode\n",
    "explodedDF = df2.withColumn(\"lang_flat\", F.explode(\"languages\"))\n",
    "explodedDF.select(\"name.firstname\",\"languages\", \"lang_flat\").show()\n",
    "\n",
    "#explode outer  \n",
    "explode_o = df2.withColumn(\"lang_flat_outer\", F.explode_outer(\"languages\"))  #-- includes null\n",
    "explode_o.select(\"name.firstname\", \"languages\", \"lang_flat_outer\").show()\n",
    "\n",
    "#pos explode\n",
    "df2.select(\"name.firstname\", \"languages\", F.posexplode(\"languages\")).show()\n",
    "\n",
    "#pos explode outer\n",
    "df2.select(\"name.firstname\", \"languages\", F.posexplode_outer(\"languages\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.sumfunc(x)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|sumofsalary(salary)|\n",
      "+-------------------+\n",
      "|              40000|\n",
      "|              30000|\n",
      "|              20000|\n",
      "|              40000|\n",
      "+-------------------+\n",
      "\n",
      "+---------------+\n",
      "|sumfunc(salary)|\n",
      "+---------------+\n",
      "|          40000|\n",
      "|          30000|\n",
      "|          20000|\n",
      "+---------------+\n",
      "\n",
      "+-------------------+------------------+-----+------+------+----------+---------------+\n",
      "|               name|         languages|state|gender|salary| createdAt|Upper_case_name|\n",
      "+-------------------+------------------+-----+------+------+----------+---------------+\n",
      "|   [James, , Smith]|[Java, Scala, C++]|   OH|     M| 20000|2019-12-01|          JAMES|\n",
      "|     [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F| 15000|2019-12-14|           ANNA|\n",
      "|[Julia, , Williams]|      [CSharp, VB]|   OH|     F| 10000|2019-12-16|          JULIA|\n",
      "+-------------------+------------------+-----+------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Register functions - UDF\n",
    "def sumfunc(x):\n",
    "    return x + x\n",
    "        \n",
    "from pyspark.sql.types import IntegerType \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df2.createOrReplaceTempView(\"employee\")\n",
    "\n",
    "#Registering function as UDF\n",
    "spark.udf.register('sumofsalary', sumfunc, IntegerType())\n",
    "spark.sql('''select sumofsalary(salary) from employee''').show()\n",
    "\n",
    "\n",
    "sumofsalaryUDF = F.udf(sumfunc, IntegerType())\n",
    "df.select(sumofsalaryUDF(\"salary\")).show()\n",
    "\n",
    "\n",
    "#Creating UDF using Anotations\n",
    "@udf(returnType=StringType()) \n",
    "def upperCaseUDF(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn(\"Upper_case_name\", upperCaseUDF(F.col(\"name.firstname\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
