{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set these in user env variables to start jupyter notebook from pyspark shell\n",
    "#PYSPARK_DRIVER_PYTHON_OPTS  notebook\n",
    "#PYSPARK_DRIVER_PYTHON jupyter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.rdd import RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=sample app, master=local) created by __init__ at <ipython-input-2-cab32e354ed6>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cab32e354ed6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample app'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#sc.stop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    339\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 341\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    342\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=sample app, master=local) created by __init__ at <ipython-input-2-cab32e354ed6>:2 "
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('sample app')\n",
    "sc = SparkContext(conf = conf)\n",
    "sc\n",
    "\n",
    "#sc.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List RDD:  [('physics', 61), ('maths', 50), ('physics', 66), ('physics', 87), ('maths', 60), ('english', 65)]\n",
      "\n",
      "Tuple RDD:  [('physics', 61), ('maths', 50), ('physics', 66), ('physics', 87), ('maths', 60), ('english', 65)]\n",
      "\n",
      "Dict RDD:  ['maths', 'english', 'physics']\n"
     ]
    }
   ],
   "source": [
    "# Create RDD using parallelize\n",
    "lines = sc.parallelize(['pandas', 'i like pandas', 'panda'])\n",
    "rdd2 = sc.parallelize(['coffee', 'pandas', 'party'])\n",
    "\n",
    "def filterdata(data):\n",
    "    print(data)\n",
    "    return 'pandas' in data\n",
    "\n",
    "\n",
    "#Check instance\n",
    "isinstance(lines, RDD)\n",
    "\n",
    "listrdd = sc.parallelize(list({(\"maths\", 50),\n",
    "                              (\"maths\", 60),\n",
    "                              (\"english\", 65),\n",
    "                              (\"physics\", 66), \n",
    "                              (\"physics\", 61), \n",
    "                              (\"physics\", 87)}), \n",
    "                         1)\n",
    "print(\"\\nList RDD: \" , listrdd.collect())\n",
    "newrdd = listrdd.flatMap(lambda x: x)\n",
    "#print(newrdd.collect())\n",
    "\n",
    "tupleRdd = sc.parallelize(tuple({(\"maths\", 50),\n",
    "                              (\"maths\", 60),\n",
    "                              (\"english\", 65),\n",
    "                              (\"physics\", 66), \n",
    "                              (\"physics\", 61), \n",
    "                              (\"physics\", 87)}), 1)\n",
    "\n",
    "print(\"\\nTuple RDD: \", tupleRdd.collect())\n",
    "\n",
    "dictRdd = sc.parallelize(dict({\"maths\": 50,\"maths\": 60,\"english\": 65,\"physics\": 66,\"physics\": 61,\"physics\": 87}))\n",
    "print(\"\\nDict RDD: \", dictRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD using external data\n",
    "textfile = sc.textFile(\"C:\\\\Users\\\\EdwinVivekN\\Desktop\\logwork.txt\")\n",
    "word = textfile.filter(lambda x: 'Mongo' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>#Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter:  ['pandas', 'i like pandas']\n",
      "\n",
      "Map:  ['pandaspandas', 'i like pandasi like pandas', 'pandapanda']\n",
      "\n",
      "Flat map:  ['pandas', 'i', 'like', 'pandas', 'panda']\n",
      "\n",
      "Distinct:  ['pandas', 'i like pandas', 'panda']\n",
      "\n",
      "Union:  ['pandas', 'i like pandas', 'panda', 'coffee', 'pandas', 'party']\n",
      "\n",
      "Intersction:  ['pandas']\n",
      "\n",
      "Subtract:  ['panda', 'i like pandas']\n",
      "\n",
      "Cartesian:  [('pandas', 'coffee'), ('pandas', 'pandas'), ('pandas', 'party'), ('i like pandas', 'coffee'), ('i like pandas', 'pandas'), ('i like pandas', 'party'), ('panda', 'coffee'), ('panda', 'pandas'), ('panda', 'party')]\n"
     ]
    }
   ],
   "source": [
    "filteredlines = lines.filter(filterdata)\n",
    "print('Filter: ', filteredlines.collect())\n",
    "\n",
    "map = lines.map(lambda x: x + x)\n",
    "print('\\nMap: ', map.collect())\n",
    "\n",
    "fmap = lines.flatMap(lambda x: x.split(' '))\n",
    "print('\\nFlat map: ', fmap.collect())\n",
    "\n",
    "print('\\nDistinct: ', lines.distinct().collect())\n",
    "\n",
    "print('\\nUnion: ',lines.union(rdd2).collect())\n",
    "print('\\nIntersction: ',lines.intersection(rdd2).collect())\n",
    "print('\\nSubtract: ',lines.subtract(rdd2).collect())\n",
    "print('\\nCartesian: ',lines.cartesian(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mongodb - testing mapreduce from local mongo server\n",
      "MongoDB connection not reachable issue details shared\n",
      "MongoDB OOM exception handling options\n",
      "MongoDB customer meeting on live connection, data warehouse and other connection ways\n",
      "MongoDB SSH code review\n",
      "MongoDB SSH connection testing \n"
     ]
    }
   ],
   "source": [
    "#sample(withReplacement, fraction, seed)\n",
    "for w in word.sample(False, 0.5).collect():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count:  14\n",
      "\n",
      "Count by value:  defaultdict(<class 'int'>, {'Mongodb - fixed issues faced in mapreduce': 1, 'Mongodb - testing mapreduce from local mongo server': 1, 'Mongodb data conversion issue fixing': 1, 'Mongodb connection issue and unauthorized issue for customer - 323957': 1})\n",
      "\n",
      "Count by key:  defaultdict(<class 'int'>, {'M': 11, 'C': 2, 'S': 1})\n",
      "\n",
      "Collect: ['M', 'M', 'M', 'C', 'C', 'M', 'M', 'M', 'M', 'M', 'S', 'M', 'M', 'M']\n"
     ]
    }
   ],
   "source": [
    "print('\\nCount: ',word.count())\n",
    "print('\\nCount by value: ',word.filter(lambda x: 'Mongodb'  in x).countByValue())\n",
    "print('\\nCollect:', word.flatMap(lambda x: x[0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top: ['SSH gathering details on MongoDB', 'Mongodb data conversion issue fixing']\n",
      "\n",
      "First: Mongodb - fixed issues faced in mapreduce\n",
      "\n",
      "Take: ['Mongodb - fixed issues faced in mapreduce', 'Mongodb - testing mapreduce from local mongo server']\n",
      "\n",
      "Take ordered: ['Checking the feasibility to expand MongoDB arrays in Bold BI ', 'Customer meeting on MongoBD related queries']\n",
      "\n",
      "Take sample: ['MongoDB dll mismatch issue in linux site', 'Checking the feasibility to expand MongoDB arrays in Bold BI ']\n"
     ]
    }
   ],
   "source": [
    "print('\\nTop:', word.top(2)) # sorted and descending (default)\n",
    "print('\\nFirst:', word.first()) #unsorted\n",
    "\n",
    "print('\\nTake:', word.take(2)) #unsorted\n",
    "print('\\nTake ordered:', word.takeOrdered(2))  # sorted and ascending (default)\n",
    "print('\\nTake sample:', word.takeSample(False, 2)) # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduce: pandas i like pandas panda\n",
      "\n",
      "Fold: X X pandas i like pandas panda\n",
      "\n",
      "Aggregate: XX pandas i like pandas panda\n"
     ]
    }
   ],
   "source": [
    "print('\\nReduce:',lines.reduce(lambda x,y: x + ' ' + y)) # returns single result\n",
    "\n",
    "print('\\nFold:',lines.fold(\"X\", lambda x,y: x + ' ' + y))  # 0 for +, 1 for *, or an empty list for concatenation\n",
    "\n",
    "seqOp = (lambda x, y: x + ' ' + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "print('\\nAggregate:', lines.aggregate((\"X\"), seqOp, combOp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6)\n",
      "\n",
      "input: [('Z', 1), ('A', 20), ('B', 60), ('C', 40)]\n",
      "('ZABC', 121)\n",
      "[('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "(' PHYSICS MATHS MATHS PHYSICS ENGLISH PHYSICS', 389)\n"
     ]
    }
   ],
   "source": [
    "#Pair Aggregate\n",
    "seqOp = (lambda x, y: (x[0] + y,  x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "pairagg = sc.parallelize([1, 2, 3, 4]).aggregate((0, 1), seqOp, combOp)\n",
    "print(pairagg)\n",
    "\n",
    "#agg on dict(string,int)\n",
    "inputRDD = sc.parallelize([dict({\"Z\": 1,\"A\": 20,\"B\": 30,\"C\": 40,\"B\": 30,\"B\": 60})])\n",
    "itemsRDD = inputRDD.flatMap(lambda x: x.items())\n",
    "print(\"\\ninput:\", itemsRDD.collect())\n",
    "seqop = lambda acc, value: (acc[0] + value[0], acc[1] + value[1])\n",
    "combop = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "result = itemsRDD.aggregate((\"\",0), seqop, combop)\n",
    "print(result)         \n",
    "\n",
    "#agg on list(string, int)\n",
    "def sequenceOp(acc, value):\n",
    "    accOp = acc[0] + \" \" + value[0]\n",
    "    valueOp = acc[1] + value[1]\n",
    "    return (accOp.upper(), valueOp)\n",
    "print(listrdd.collect())\n",
    "combop = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "agg = listrdd.aggregate((\"\",0), sequenceOp, combop)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printdata(x):\n",
    "    print(x)\n",
    "\n",
    "#foreach\n",
    "word.foreach(lambda x: print(x))\n",
    "\n",
    "#Iteration\n",
    "iterator = word.toLocalIterator()\n",
    "for i in iterator:\n",
    "    pass\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pair RDD Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mongodb', ['-', 'fixed', 'issues', 'faced', 'in', 'mapreduce']),\n",
       " ('Mongodb',\n",
       "  ['-', 'testing', 'mapreduce', 'from', 'local', 'mongo', 'server']),\n",
       " ('MongoDB', ['-', 'mapreduce', 'testing', 'in', 'local']),\n",
       " ('Checking',\n",
       "  ['the',\n",
       "   'feasibility',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'MongoDB',\n",
       "   'arrays',\n",
       "   'in',\n",
       "   'Bold',\n",
       "   'BI',\n",
       "   '']),\n",
       " ('Customer', ['meeting', 'on', 'MongoBD', 'related', 'queries']),\n",
       " ('MongoDB', ['connection', 'not', 'reachable', 'issue', 'details', 'shared']),\n",
       " ('MongoDB', ['dll', 'mismatch', 'issue', 'in', 'linux', 'site']),\n",
       " ('Mongodb', ['data', 'conversion', 'issue', 'fixing']),\n",
       " ('MongoDB', ['OOM', 'exception', 'handling', 'options']),\n",
       " ('Mongodb',\n",
       "  ['connection',\n",
       "   'issue',\n",
       "   'and',\n",
       "   'unauthorized',\n",
       "   'issue',\n",
       "   'for',\n",
       "   'customer',\n",
       "   '-',\n",
       "   '323957']),\n",
       " ('SSH', ['gathering', 'details', 'on', 'MongoDB']),\n",
       " ('MongoDB',\n",
       "  ['customer',\n",
       "   'meeting',\n",
       "   'on',\n",
       "   'live',\n",
       "   'connection,',\n",
       "   'data',\n",
       "   'warehouse',\n",
       "   'and',\n",
       "   'other',\n",
       "   'connection',\n",
       "   'ways']),\n",
       " ('MongoDB', ['SSH', 'code', 'review']),\n",
       " ('MongoDB', ['SSH', 'connection', 'testing', ''])]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating pair RDD\n",
    "wordpair = word.map(lambda x: (x.split(\" \")[0], x.split(\" \")[1]))\n",
    "wordpair.collect()\n",
    "\n",
    "def splitfunc(word):\n",
    "    key = word.split(\" \")[0]\n",
    "    value = word.split(\" \")[1:]\n",
    "    return [key, value] #returns list\n",
    "\n",
    "wordpair2 = word.map(lambda x: (splitfunc(x)[0], splitfunc(x)[1]))\n",
    "wordpair2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduce By Key:  [('Mongodb', '- - data connection'), ('MongoDB', '- connection dll OOM customer SSH SSH'), ('Checking', 'the'), ('Customer', 'meeting'), ('SSH', 'gathering')]\n",
      "\n",
      "Group By Key:  [('Mongodb', <pyspark.resultiterable.ResultIterable object at 0x04C28F10>), ('MongoDB', <pyspark.resultiterable.ResultIterable object at 0x04C28F90>), ('Checking', <pyspark.resultiterable.ResultIterable object at 0x04C70070>), ('Customer', <pyspark.resultiterable.ResultIterable object at 0x04C70050>), ('SSH', <pyspark.resultiterable.ResultIterable object at 0x04C70110>)]\n",
      "\n",
      "Map Values: [('Mongodb', '- new val'), ('Mongodb', '- new val'), ('MongoDB', '- new val'), ('Checking', 'the new val'), ('Customer', 'meeting new val'), ('MongoDB', 'connection new val'), ('MongoDB', 'dll new val'), ('Mongodb', 'data new val'), ('MongoDB', 'OOM new val'), ('Mongodb', 'connection new val'), ('SSH', 'gathering new val'), ('MongoDB', 'customer new val'), ('MongoDB', 'SSH new val'), ('MongoDB', 'SSH new val')]\n",
      "\n",
      "Flap Map Values: [('Mongodb', '- - d a t a c o n n'), ('MongoDB', '- c o n n d l l O O M c u s t S S H S S H'), ('Checking', 't h e'), ('Customer', 'm e e t'), ('SSH', 'g a t h')]\n",
      "\n",
      "Keys:  ['Mongodb', 'Mongodb', 'MongoDB', 'Checking', 'Customer', 'MongoDB', 'MongoDB', 'Mongodb', 'MongoDB', 'Mongodb', 'SSH', 'MongoDB', 'MongoDB', 'MongoDB']\n",
      "\n",
      "Values:  ['-', '-', '-', 'the', 'meeting', 'connection', 'dll', 'data', 'OOM', 'connection', 'gathering', 'customer', 'SSH', 'SSH']\n",
      "\n",
      "Sort By Key:  [('Checking', 'the'), ('Customer', 'meeting'), ('MongoDB', '-'), ('MongoDB', 'connection'), ('MongoDB', 'dll'), ('MongoDB', 'OOM'), ('MongoDB', 'customer'), ('MongoDB', 'SSH'), ('MongoDB', 'SSH'), ('Mongodb', '-'), ('Mongodb', '-'), ('Mongodb', 'data'), ('Mongodb', 'connection'), ('SSH', 'gathering')]\n"
     ]
    }
   ],
   "source": [
    "print('\\nMap Values:', wordpair.mapValues(lambda x: x + \" new val\").collect())\n",
    "print('\\nFlap Map Values:', wordpair.flatMapValues(lambda x: x[0:4]).reduceByKey(lambda x,y: x +\" \"+ y).collect())\n",
    "print('\\nKeys: ', wordpair.keys().collect())\n",
    "print('\\nValues: ', wordpair.values().collect())\n",
    "print('\\nReduce By Key: ', wordpair.reduceByKey(lambda x,y: x +\" \"+ y).collect())\n",
    "print('\\nFold By Key: ', wordpair.foldByKey(\"XX\", lambda x,y: x +\" \"+ y).collect())\n",
    "print('\\nGroup By Key: ', wordpair.groupByKey().collect())\n",
    "print('\\nSort By Key: ', wordpair.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "[('physics', 214), ('maths', 110), ('english', 65)]\n",
      "[('physics', (214, 3)), ('maths', (110, 2)), ('english', (65, 1))]\n"
     ]
    }
   ],
   "source": [
    "#combineByKey\n",
    "print(listrdd.collect())\n",
    "\n",
    "combine = listrdd.combineByKey(lambda v: v, lambda v1,v2: v1+v2, lambda c1,c2: c1+c2)\n",
    "print(combine.collect())\n",
    "\n",
    "def createCombiner(v):   #initial values of each keys only are passed here, not keys\n",
    "    return (v, 1)\n",
    "\n",
    "def mergeValues(v1, v2):  #values are merged here\n",
    "    return (v1[0] + v2, v1[1] + 1)\n",
    "\n",
    "def mergeCombiners(c1, c2):  #values are combined here\n",
    "    return (c1[0] + c2[0], c1[1]+ c2[1])\n",
    "    \n",
    "\n",
    "combine2 = listrdd.combineByKey(createCombiner, mergeValues, mergeCombiners)\n",
    "print(combine2.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('physics', 214), ('maths', 110), ('english', 65)]\n"
     ]
    }
   ],
   "source": [
    "#aggregateByKey\n",
    "seqop = lambda acc, value: (acc + value)\n",
    "combop = lambda acc1, acc2: (acc1 + acc2)  #acc1[0] + acc2[0], acc1[1] + acc2[1]\n",
    "aggbykey = listrdd.aggregateByKey(0, seqop , combop)\n",
    "print(aggbykey.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mongodb', <pyspark.resultiterable.ResultIterable at 0xba56f90>),\n",
       " ('MongoDB', <pyspark.resultiterable.ResultIterable at 0xba56450>),\n",
       " ('Checking', <pyspark.resultiterable.ResultIterable at 0xba561f0>),\n",
       " ('Customer', <pyspark.resultiterable.ResultIterable at 0xba56430>),\n",
       " ('SSH', <pyspark.resultiterable.ResultIterable at 0xba562d0>)]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbk = wordpair.groupByKey()\n",
    "gbk.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subtract By Key:  [('english', 65)]\n",
      "\n",
      "Join:  [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20))]\n",
      "\n",
      "left outer join:  [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20)), ('english', (65, None))]\n",
      "\n",
      "right outer join:  [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20))]\n",
      "\n",
      "cogroup:  [('physics', (<pyspark.resultiterable.ResultIterable object at 0x0B61C1F0>, <pyspark.resultiterable.ResultIterable object at 0x0B61CA70>)), ('maths', (<pyspark.resultiterable.ResultIterable object at 0x0B61CC70>, <pyspark.resultiterable.ResultIterable object at 0x0B61C970>)), ('english', (<pyspark.resultiterable.ResultIterable object at 0x0B61CFD0>, <pyspark.resultiterable.ResultIterable object at 0x0B61C070>))]\n"
     ]
    }
   ],
   "source": [
    "otherListrdd = sc.parallelize(list({(\"maths\", 20),\n",
    "                              (\"maths\", 100),\n",
    "                              (\"physics\", 66), \n",
    "                              (\"physics\", 90)}), \n",
    "                         1)\n",
    "\n",
    "print('\\nSubtract By Key: ', listrdd.subtractByKey(otherListrdd).collect())\n",
    "\n",
    "print('\\nJoin: ', listrdd.join(otherListrdd).collect())\n",
    "\n",
    "print(\"\\nleft outer join: \", listrdd.leftOuterJoin(otherListrdd).collect())\n",
    "\n",
    "print(\"\\nright outer join: \", listrdd.rightOuterJoin(otherListrdd).collect())\n",
    "\n",
    "print(\"\\ncogroup: \",  listrdd.cogroup(otherListrdd).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1: [('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "Input2: [('maths', 100), ('physics', 66), ('physics', 90), ('maths', 20)]\n",
      "\n",
      "Join: [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20))]\n",
      "\n",
      "Join+MapValues: [('physics', 132), ('physics', 156), ('physics', 127), ('physics', 151), ('physics', 153), ('physics', 177), ('maths', 150), ('maths', 70), ('maths', 160), ('maths', 80)]\n",
      "\n",
      "Join+Map+Agg: [('physics', 896), ('maths', 460)]\n"
     ]
    }
   ],
   "source": [
    "#Join + MapValues + AggregateByKey\n",
    "print(\"Input1:\", inputrdd.collect())\n",
    "print(\"Input2:\", otherListrdd.collect())\n",
    "joinrdd = listrdd.join(otherListrdd)\n",
    "print(\"\\nJoin:\", joinrdd.collect())\n",
    "maprdd = joinrdd.mapValues(lambda x: x[0] + x[1])\n",
    "print(\"\\nJoin+MapValues:\", maprdd.collect())\n",
    "joinMapAgg = maprdd.aggregateByKey(0, lambda x,y: x + y, lambda x,y: x + y)\n",
    "print(\"\\nJoin+Map+Agg:\", joinMapAgg.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pair RDD Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count by key:  defaultdict(<class 'int'>, {'Mongodb': 4, 'MongoDB': 7, 'Checking': 1, 'Customer': 1, 'SSH': 1})\n",
      "\n",
      "Collect As Map:  {'Mongodb': 'connection', 'MongoDB': 'SSH', 'Checking': 'the', 'Customer': 'meeting', 'SSH': 'gathering'}\n",
      "\n",
      "Lookup ['-', 'connection', 'dll', 'OOM', 'customer', 'SSH', 'SSH']\n"
     ]
    }
   ],
   "source": [
    "print('\\nCount by key: ',wordpair.countByKey())\n",
    "print('\\nCollect As Map: ', wordpair.collectAsMap()) #if there are multiple keys with different values, then collectASMap() will collect by returning the updated value with respect to the key\n",
    "print('\\nLookup', wordpair.lookup('MongoDB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persisting \n",
    "\n",
    "#word.persist() or word.cache()\n",
    "#word.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Partitioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[144] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize(['pandas', 'i like pandas', 'panda', 'python'], 3)\n",
    "lines.getNumPartitions()\n",
    "\n",
    "def toCSV(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "csvlines = lines.map(toCSV)\n",
    "csvlines\n",
    "csvlines.saveAsTextFile(\"sample_parts.csv\")   #save as part files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "|       pandas|\n",
      "|i like pandas|\n",
      "|        panda|\n",
      "|       python|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "df = lines.toDF(schema=StringType())\n",
    "df.show()\n",
    "df.write.mode(\"overwrite\").csv(\"sample\", header=True) #save as part file csv\n",
    "\n",
    "df2 = csvlines.toDF(schema=StringType())\n",
    "df2.show()\n",
    "df2.write.mode(\"overwrite\").csv(\"sample2\", header=True) #save as part file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#reduce the partitioning\n",
    "df3 = df.coalesce(1)\n",
    "df3.write.mode(\"overwrite\").csv(\"sample3\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('July', 2),\n",
       " ('20', 3),\n",
       " ('2h', 1),\n",
       " ('Powertech', 4),\n",
       " ('customer', 5),\n",
       " ('acc', 1),\n",
       " ('testing', 7),\n",
       " ('', 431),\n",
       " ('21', 3),\n",
       " ('1h', 1),\n",
       " ('live', 5),\n",
       " ('mode', 1),\n",
       " ('missing', 1),\n",
       " ('data', 8),\n",
       " ('-', 22),\n",
       " ('updated', 5),\n",
       " ('not', 5),\n",
       " ('reproduced', 1),\n",
       " ('23', 4),\n",
       " ('3h', 3),\n",
       " ('Need', 1),\n",
       " ('to', 6),\n",
       " ('replicate', 1),\n",
       " ('web', 7),\n",
       " ('API', 3),\n",
       " ('issue', 23),\n",
       " ('Generate', 1),\n",
       " ('logs', 2),\n",
       " ('for', 24),\n",
       " ('and', 25),\n",
       " ('sqlite', 1),\n",
       " ('connection', 13),\n",
       " ('in', 12),\n",
       " ('build', 1),\n",
       " ('24', 4),\n",
       " ('6h', 2),\n",
       " ('api', 4),\n",
       " ('replication', 2),\n",
       " ('27', 2),\n",
       " ('0h', 1),\n",
       " ('Web', 2),\n",
       " ('powertech', 1),\n",
       " ('Aug', 2),\n",
       " ('6', 2),\n",
       " ('Jira', 2),\n",
       " ('basic', 3),\n",
       " ('kpi', 1),\n",
       " ('investigation', 13),\n",
       " ('Attended', 2),\n",
       " ('meeting', 6),\n",
       " ('create', 1),\n",
       " ('dashboards', 2),\n",
       " ('Sep', 2),\n",
       " ('29', 3),\n",
       " ('Mongodb', 4),\n",
       " ('fixed', 7),\n",
       " ('issues', 4),\n",
       " ('faced', 1),\n",
       " ('mapreduce', 3),\n",
       " ('from', 1),\n",
       " ('local', 2),\n",
       " ('mongo', 3),\n",
       " ('server', 1),\n",
       " ('30', 3),\n",
       " ('Basic', 1),\n",
       " ('on', 24),\n",
       " ('CosmosDB', 1),\n",
       " ('MongoDB', 9),\n",
       " ('Elaticsearch', 1),\n",
       " ('continuing', 1),\n",
       " ('source', 4),\n",
       " ('Updated', 7),\n",
       " ('response', 6),\n",
       " ('#294276', 2),\n",
       " ('#295610', 1),\n",
       " ('Oct', 1),\n",
       " ('15', 2),\n",
       " ('Bug', 1),\n",
       " ('Refresh', 1),\n",
       " ('tables', 1),\n",
       " ('code', 4),\n",
       " ('view', 2),\n",
       " ('failed', 1),\n",
       " ('Shared', 6),\n",
       " ('#296520', 1),\n",
       " (',', 2),\n",
       " ('#296338', 1),\n",
       " ('#295053', 1),\n",
       " ('Amazon', 1),\n",
       " ('Cloud', 1),\n",
       " ('watch', 1),\n",
       " ('Checking', 1),\n",
       " ('the', 1),\n",
       " ('feasibility', 1),\n",
       " ('expand', 1),\n",
       " ('arrays', 2),\n",
       " ('Bold', 1),\n",
       " ('BI', 1),\n",
       " ('scrum', 1),\n",
       " ('Customer', 2),\n",
       " ('column', 2),\n",
       " ('duplication', 1),\n",
       " ('MongoBD', 1),\n",
       " ('related', 1),\n",
       " ('queries', 1),\n",
       " ('Published', 1),\n",
       " ('bold', 2),\n",
       " ('bi', 2),\n",
       " ('linux', 5),\n",
       " ('site', 6),\n",
       " ('Facing', 1),\n",
       " ('object', 1),\n",
       " ('ref', 1),\n",
       " ('all', 2),\n",
       " ('connectors', 2),\n",
       " ('Nov', 1),\n",
       " ('12', 4),\n",
       " ('https://syncfusion.atlassian.net/browse/DASHBRD-47795', 2),\n",
       " ('.net', 15),\n",
       " ('core', 15),\n",
       " ('edit', 4),\n",
       " ('with', 4),\n",
       " ('different', 2),\n",
       " ('columns', 3),\n",
       " ('throw', 2),\n",
       " ('empty', 2),\n",
       " ('preview', 4),\n",
       " ('Checked', 2),\n",
       " ('alemba', 2),\n",
       " ('utility', 2),\n",
       " ('shared', 2),\n",
       " ('it', 1),\n",
       " ('#297710', 1),\n",
       " ('c#', 1),\n",
       " ('13', 1),\n",
       " ('16', 3),\n",
       " ('elasticsearch', 1),\n",
       " ('#295406', 1),\n",
       " ('17', 2),\n",
       " ('oauth', 7),\n",
       " ('configuration', 1),\n",
       " ('set', 1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-designer/merge_requests/6612/diffs',\n",
       "  1),\n",
       " ('18', 4),\n",
       " ('fixing', 4),\n",
       " ('details', 8),\n",
       " ('OAuth', 6),\n",
       " ('19', 3),\n",
       " ('QUickbooks', 1),\n",
       " ('desktop', 2),\n",
       " ('cloud', 2),\n",
       " ('&', 3),\n",
       " ('file', 1),\n",
       " ('ds', 1),\n",
       " ('working', 1),\n",
       " ('Quickbooks', 1),\n",
       " ('#', 1),\n",
       " ('304001', 1),\n",
       " ('fix', 3),\n",
       " ('linux-beta', 1),\n",
       " ('fail', 1),\n",
       " ('connect', 1),\n",
       " ('https://syncfusion.atlassian.net/browse/DASHBRD-48230', 2),\n",
       " ('refresh', 7),\n",
       " ('issue,', 1),\n",
       " ('port', 1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-service/merge_requests/907',\n",
       "  1),\n",
       " ('https://syncfusion.atlassian.net/browse/DASHBRD-48233', 1),\n",
       " ('mongodb', 11),\n",
       " ('plugin', 1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-connection-mongodb-plugin/merge_requests/23/diffs',\n",
       "  1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-connection-mongodb-model/merge_requests/63/diffs',\n",
       "  1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-base/merge_requests/2792',\n",
       "  1),\n",
       " ('25', 3),\n",
       " ('path', 1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-service/merge_requests/913/diffs',\n",
       "  1),\n",
       " ('https://syncfusion.atlassian.net/browse/DASHBRD-48006', 2),\n",
       " ('no', 2),\n",
       " ('generated', 1),\n",
       " ('checking', 3),\n",
       " ('designer', 2),\n",
       " ('hub', 1),\n",
       " ('console', 1),\n",
       " ('error', 2),\n",
       " ('tested', 1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-service/merge_requests/921',\n",
       "  1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-base/merge_requests/2812',\n",
       "  1),\n",
       " ('Dec', 1),\n",
       " ('22', 2),\n",
       " ('Generic', 2),\n",
       " ('rfc', 1),\n",
       " ('specification', 1),\n",
       " ('prepared', 1),\n",
       " ('work', 2),\n",
       " ('split', 1),\n",
       " ('up', 1),\n",
       " ('31', 2),\n",
       " ('postman', 2),\n",
       " ('integration', 1),\n",
       " ('validated', 1),\n",
       " ('#288502', 1),\n",
       " ('template', 1),\n",
       " ('jan', 1),\n",
       " ('4h', 3),\n",
       " ('generic', 1),\n",
       " ('28', 3),\n",
       " ('Open', 2),\n",
       " ('ID', 2),\n",
       " ('#312639', 1),\n",
       " ('openID', 1),\n",
       " ('invetgation', 1),\n",
       " ('ap', 1),\n",
       " ('https://gitlab.syncfusion.com/data-science/dashboard-designer-web-data-handler/merge_requests/720/diffs',\n",
       "  1),\n",
       " ('feb', 1),\n",
       " ('1', 2),\n",
       " ('5h', 2),\n",
       " ('openid', 1),\n",
       " ('JWT', 3),\n",
       " ('token', 2),\n",
       " ('authentication', 2),\n",
       " ('inestigation', 1),\n",
       " ('relevant', 1),\n",
       " ('datasource', 1),\n",
       " ('Token', 1),\n",
       " ('extract', 1),\n",
       " ('development', 2),\n",
       " ('reachable', 1),\n",
       " ('#315973', 1),\n",
       " ('steps', 1),\n",
       " ('dll', 1),\n",
       " ('mismatch', 1),\n",
       " ('google', 1),\n",
       " ('cecking', 1),\n",
       " ('document', 1),\n",
       " ('db', 1),\n",
       " ('sll', 1),\n",
       " ('subnet', 1),\n",
       " ('nested', 1),\n",
       " ('quickbooks', 1),\n",
       " ('drop', 2),\n",
       " ('after', 1),\n",
       " ('few', 1),\n",
       " ('days', 1),\n",
       " ('#316340', 2),\n",
       " ('#316318', 1),\n",
       " ('qbo', 2),\n",
       " ('staging', 1),\n",
       " ('MOngoDB', 1),\n",
       " ('flat', 2),\n",
       " ('table', 2),\n",
       " ('conversion', 2),\n",
       " ('array', 1),\n",
       " ('documents', 1),\n",
       " ('#316803,', 1),\n",
       " ('#316340,', 1),\n",
       " ('#316825', 1),\n",
       " ('#316787', 1),\n",
       " ('Mar', 1),\n",
       " ('8', 1),\n",
       " ('Fixed', 1),\n",
       " ('OOM', 3),\n",
       " ('due', 1),\n",
       " ('#318125', 1),\n",
       " ('azure', 3),\n",
       " ('devops', 2),\n",
       " ('9', 1),\n",
       " ('exception', 2),\n",
       " ('handling', 2),\n",
       " ('options', 1),\n",
       " ('Investigation', 2),\n",
       " ('json', 4),\n",
       " ('base', 4),\n",
       " ('Investigating', 1),\n",
       " ('competitors', 1),\n",
       " ('how', 1),\n",
       " ('is', 1),\n",
       " ('handled', 1),\n",
       " ('Apr', 1),\n",
       " ('rest', 1),\n",
       " ('returns', 1),\n",
       " ('+2h', 1),\n",
       " ('unauthorized', 1),\n",
       " ('323957', 1),\n",
       " ('#323957', 1),\n",
       " ('#321553', 1),\n",
       " ('documentdb', 1),\n",
       " ('May', 1),\n",
       " ('Reproducing', 1),\n",
       " ('found', 1),\n",
       " ('CSV', 1),\n",
       " ('7', 2),\n",
       " ('SSH', 5),\n",
       " ('gathering', 1),\n",
       " ('11', 1),\n",
       " ('Vs', 1),\n",
       " ('runtime', 1),\n",
       " ('debugging', 1),\n",
       " ('of', 1),\n",
       " ('sorurce', 1),\n",
       " ('dB', 1),\n",
       " ('June', 1),\n",
       " ('14', 1),\n",
       " ('Json', 3),\n",
       " ('Base', 1),\n",
       " ('row', 3),\n",
       " ('count', 3),\n",
       " ('before', 2),\n",
       " ('datatable', 1),\n",
       " ('fetch', 1),\n",
       " ('panoimagen', 1),\n",
       " ('ssh', 1),\n",
       " ('cases', 1),\n",
       " ('calculate', 2),\n",
       " ('#330143', 1),\n",
       " ('slowness', 1),\n",
       " ('DT', 1),\n",
       " ('preparation', 1),\n",
       " ('connection,', 2),\n",
       " ('warehouse', 1),\n",
       " ('other', 1),\n",
       " ('ways', 1),\n",
       " ('Preview', 1),\n",
       " ('UI', 2),\n",
       " ('type', 1),\n",
       " ('types', 1),\n",
       " ('#334337', 1),\n",
       " ('#302152', 1),\n",
       " ('oom', 1),\n",
       " ('5', 1),\n",
       " ('MongDB', 1),\n",
       " ('improvements', 1),\n",
       " ('listing', 1),\n",
       " ('tasks', 1),\n",
       " ('splitup', 1),\n",
       " ('review', 1),\n",
       " ('#327233', 1),\n",
       " ('\\t', 1),\n",
       " ('SignalR', 1),\n",
       " ('scaleout', 1),\n",
       " ('11,', 1),\n",
       " ('reauthorize,', 1),\n",
       " ('grant', 1),\n",
       " ('type,', 1),\n",
       " ('access', 1),\n",
       " ('signalR', 1),\n",
       " ('service', 1),\n",
       " ('https://support.boldbi.com/agent/tickets/11635', 1),\n",
       " ('freshsales', 1),\n",
       " ('incident', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word count sample\n",
    "textfile = sc.textFile(\"C:\\\\Users\\\\EdwinVivekN\\Desktop\\logwork.txt\")\n",
    "words = textfile.flatMap(lambda lines: lines.split(\" \"))\n",
    "wordPair = words.map(lambda word: (word, 1))\n",
    "wordCount = wordPair.reduceByKey(lambda v1, v2: v1+v2)\n",
    "wordCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list dependency jars\n",
    "jars = sc._jsc.sc().listJars()\n",
    "print(jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
