{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.rdd import RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://synclapn17855.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sample app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=sample app>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('sample app')\n",
    "sc = SparkContext(conf = conf)\n",
    "sc\n",
    "\n",
    "#sc.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List RDD:  [('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "\n",
      "Tuple RDD:  [('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "\n",
      "Dict RDD:  ['maths', 'english', 'physics']\n"
     ]
    }
   ],
   "source": [
    "# Create RDD using parallelize\n",
    "lines = sc.parallelize(['pandas', 'i like pandas', 'panda'])\n",
    "rdd2 = sc.parallelize(['coffee', 'pandas', 'party'])\n",
    "\n",
    "def filterdata(data):\n",
    "    print(data)\n",
    "    return 'pandas' in data\n",
    "\n",
    "\n",
    "#Check instance\n",
    "isinstance(lines, RDD)\n",
    "\n",
    "listrdd = sc.parallelize(list({(\"maths\", 50),\n",
    "                              (\"maths\", 60),\n",
    "                              (\"english\", 65),\n",
    "                              (\"physics\", 66), \n",
    "                              (\"physics\", 61), \n",
    "                              (\"physics\", 87)}), \n",
    "                         1)\n",
    "print(\"\\nList RDD: \" , listrdd.collect())\n",
    "newrdd = listrdd.flatMap(lambda x: x)\n",
    "#print(newrdd.collect())\n",
    "\n",
    "tupleRdd = sc.parallelize(tuple({(\"maths\", 50),\n",
    "                              (\"maths\", 60),\n",
    "                              (\"english\", 65),\n",
    "                              (\"physics\", 66), \n",
    "                              (\"physics\", 61), \n",
    "                              (\"physics\", 87)}), 1)\n",
    "\n",
    "print(\"\\nTuple RDD: \", tupleRdd.collect())\n",
    "\n",
    "dictRdd = sc.parallelize(dict({\"maths\": 50,\"maths\": 60,\"english\": 65,\"physics\": 66,\"physics\": 61,\"physics\": 87}))\n",
    "print(\"\\nDict RDD: \", dictRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD using external data\n",
    "textfile = sc.textFile(\"C:\\\\Users\\\\EdwinVivekN\\Desktop\\logwork.txt\")\n",
    "word = textfile.filter(lambda x: 'Mongo' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>#Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter:  ['pandas', 'i like pandas']\n",
      "\n",
      "Map:  ['pandaspandas', 'i like pandasi like pandas', 'pandapanda']\n",
      "\n",
      "Flat map:  ['pandas', 'i', 'like', 'pandas', 'panda']\n",
      "\n",
      "Distinct:  ['pandas', 'i like pandas', 'panda']\n",
      "\n",
      "Union:  ['pandas', 'i like pandas', 'panda', 'coffee', 'pandas', 'party']\n",
      "\n",
      "Intersction:  ['pandas']\n",
      "\n",
      "Subtract:  ['panda', 'i like pandas']\n",
      "\n",
      "Cartesian:  [('pandas', 'coffee'), ('pandas', 'pandas'), ('pandas', 'party'), ('i like pandas', 'coffee'), ('i like pandas', 'pandas'), ('i like pandas', 'party'), ('panda', 'coffee'), ('panda', 'pandas'), ('panda', 'party')]\n"
     ]
    }
   ],
   "source": [
    "filteredlines = lines.filter(filterdata)\n",
    "print('Filter: ', filteredlines.collect())\n",
    "\n",
    "map = lines.map(lambda x: x + x)\n",
    "print('\\nMap: ', map.collect())\n",
    "\n",
    "fmap = lines.flatMap(lambda x: x.split(' '))\n",
    "print('\\nFlat map: ', fmap.collect())\n",
    "\n",
    "print('\\nDistinct: ', lines.distinct().collect())\n",
    "\n",
    "print('\\nUnion: ',lines.union(rdd2).collect())\n",
    "print('\\nIntersction: ',lines.intersection(rdd2).collect())\n",
    "print('\\nSubtract: ',lines.subtract(rdd2).collect())\n",
    "print('\\nCartesian: ',lines.cartesian(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mongodb - testing mapreduce from local mongo server\n",
      "MongoDB connection not reachable issue details shared\n",
      "MongoDB OOM exception handling options\n",
      "MongoDB customer meeting on live connection, data warehouse and other connection ways\n",
      "MongoDB SSH code review\n",
      "MongoDB SSH connection testing \n"
     ]
    }
   ],
   "source": [
    "#sample(withReplacement, fraction, seed)\n",
    "for w in word.sample(False, 0.5).collect():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count:  14\n",
      "\n",
      "Count by value:  defaultdict(<class 'int'>, {'Mongodb - fixed issues faced in mapreduce': 1, 'Mongodb - testing mapreduce from local mongo server': 1, 'Mongodb data conversion issue fixing': 1, 'Mongodb connection issue and unauthorized issue for customer - 323957': 1})\n",
      "\n",
      "Count by key:  defaultdict(<class 'int'>, {'M': 11, 'C': 2, 'S': 1})\n",
      "\n",
      "Collect: ['M', 'M', 'M', 'C', 'C', 'M', 'M', 'M', 'M', 'M', 'S', 'M', 'M', 'M']\n"
     ]
    }
   ],
   "source": [
    "print('\\nCount: ',word.count())\n",
    "print('\\nCount by value: ',word.filter(lambda x: 'Mongodb'  in x).countByValue())\n",
    "print('\\nCollect:', word.flatMap(lambda x: x[0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top: ['SSH gathering details on MongoDB', 'Mongodb data conversion issue fixing']\n",
      "\n",
      "First: Mongodb - fixed issues faced in mapreduce\n",
      "\n",
      "Take: ['Mongodb - fixed issues faced in mapreduce', 'Mongodb - testing mapreduce from local mongo server']\n",
      "\n",
      "Take ordered: ['Checking the feasibility to expand MongoDB arrays in Bold BI ', 'Customer meeting on MongoBD related queries']\n",
      "\n",
      "Take sample: ['MongoDB dll mismatch issue in linux site', 'Checking the feasibility to expand MongoDB arrays in Bold BI ']\n"
     ]
    }
   ],
   "source": [
    "print('\\nTop:', word.top(2)) # sorted and descending (default)\n",
    "print('\\nFirst:', word.first()) #unsorted\n",
    "\n",
    "print('\\nTake:', word.take(2)) #unsorted\n",
    "print('\\nTake ordered:', word.takeOrdered(2))  # sorted and ascending (default)\n",
    "print('\\nTake sample:', word.takeSample(False, 2)) # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduce: pandas i like pandas panda\n",
      "\n",
      "Fold: X X pandas i like pandas panda\n",
      "\n",
      "Aggregate: XX pandas i like pandas panda\n"
     ]
    }
   ],
   "source": [
    "print('\\nReduce:',lines.reduce(lambda x,y: x + ' ' + y)) # returns single result\n",
    "\n",
    "print('\\nFold:',lines.fold(\"X\", lambda x,y: x + ' ' + y))  # 0 for +, 1 for *, or an empty list for concatenation\n",
    "\n",
    "seqOp = (lambda x, y: x + ' ' + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "print('\\nAggregate:', lines.aggregate((\"X\"), seqOp, combOp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6)\n",
      "\n",
      "input: [('Z', 1), ('A', 20), ('B', 60), ('C', 40)]\n",
      "('ZABC', 121)\n",
      "[('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "(' PHYSICS MATHS MATHS PHYSICS ENGLISH PHYSICS', 389)\n"
     ]
    }
   ],
   "source": [
    "#Pair Aggregate\n",
    "seqOp = (lambda x, y: (x[0] + y,  x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "pairagg = sc.parallelize([1, 2, 3, 4]).aggregate((0, 1), seqOp, combOp)\n",
    "print(pairagg)\n",
    "\n",
    "#agg on dict(string,int)\n",
    "inputRDD = sc.parallelize([dict({\"Z\": 1,\"A\": 20,\"B\": 30,\"C\": 40,\"B\": 30,\"B\": 60})])\n",
    "itemsRDD = inputRDD.flatMap(lambda x: x.items())\n",
    "print(\"\\ninput:\", itemsRDD.collect())\n",
    "seqop = lambda acc, value: (acc[0] + value[0], acc[1] + value[1])\n",
    "combop = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "result = itemsRDD.aggregate((\"\",0), seqop, combop)\n",
    "print(result)         \n",
    "\n",
    "#agg on list(string, int)\n",
    "def sequenceOp(acc, value):\n",
    "    accOp = acc[0] + \" \" + value[0]\n",
    "    valueOp = acc[1] + value[1]\n",
    "    return (accOp.upper(), valueOp)\n",
    "print(listrdd.collect())\n",
    "combop = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "agg = listrdd.aggregate((\"\",0), sequenceOp, combop)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printdata(x):\n",
    "    print(x)\n",
    "\n",
    "#foreach\n",
    "word.foreach(lambda x: print(x))\n",
    "\n",
    "#Iteration\n",
    "iterator = word.toLocalIterator()\n",
    "for i in iterator:\n",
    "    pass\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pair RDD Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mongodb', ['-', 'fixed', 'issues', 'faced', 'in', 'mapreduce']),\n",
       " ('Mongodb',\n",
       "  ['-', 'testing', 'mapreduce', 'from', 'local', 'mongo', 'server']),\n",
       " ('MongoDB', ['-', 'mapreduce', 'testing', 'in', 'local']),\n",
       " ('Checking',\n",
       "  ['the',\n",
       "   'feasibility',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'MongoDB',\n",
       "   'arrays',\n",
       "   'in',\n",
       "   'Bold',\n",
       "   'BI',\n",
       "   '']),\n",
       " ('Customer', ['meeting', 'on', 'MongoBD', 'related', 'queries']),\n",
       " ('MongoDB', ['connection', 'not', 'reachable', 'issue', 'details', 'shared']),\n",
       " ('MongoDB', ['dll', 'mismatch', 'issue', 'in', 'linux', 'site']),\n",
       " ('Mongodb', ['data', 'conversion', 'issue', 'fixing']),\n",
       " ('MongoDB', ['OOM', 'exception', 'handling', 'options']),\n",
       " ('Mongodb',\n",
       "  ['connection',\n",
       "   'issue',\n",
       "   'and',\n",
       "   'unauthorized',\n",
       "   'issue',\n",
       "   'for',\n",
       "   'customer',\n",
       "   '-',\n",
       "   '323957']),\n",
       " ('SSH', ['gathering', 'details', 'on', 'MongoDB']),\n",
       " ('MongoDB',\n",
       "  ['customer',\n",
       "   'meeting',\n",
       "   'on',\n",
       "   'live',\n",
       "   'connection,',\n",
       "   'data',\n",
       "   'warehouse',\n",
       "   'and',\n",
       "   'other',\n",
       "   'connection',\n",
       "   'ways']),\n",
       " ('MongoDB', ['SSH', 'code', 'review']),\n",
       " ('MongoDB', ['SSH', 'connection', 'testing', ''])]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating pair RDD\n",
    "wordpair = word.map(lambda x: (x.split(\" \")[0], x.split(\" \")[1]))\n",
    "wordpair.collect()\n",
    "\n",
    "def splitfunc(word):\n",
    "    key = word.split(\" \")[0]\n",
    "    value = word.split(\" \")[1:]\n",
    "    return [key, value] #returns list\n",
    "\n",
    "wordpair2 = word.map(lambda x: (splitfunc(x)[0], splitfunc(x)[1]))\n",
    "wordpair2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduce By Key:  [('Mongodb', '- - data connection'), ('MongoDB', '- connection dll OOM customer SSH SSH'), ('Checking', 'the'), ('Customer', 'meeting'), ('SSH', 'gathering')]\n",
      "\n",
      "Group By Key:  [('Mongodb', <pyspark.resultiterable.ResultIterable object at 0x04C28F10>), ('MongoDB', <pyspark.resultiterable.ResultIterable object at 0x04C28F90>), ('Checking', <pyspark.resultiterable.ResultIterable object at 0x04C70070>), ('Customer', <pyspark.resultiterable.ResultIterable object at 0x04C70050>), ('SSH', <pyspark.resultiterable.ResultIterable object at 0x04C70110>)]\n",
      "\n",
      "Map Values: [('Mongodb', '- new val'), ('Mongodb', '- new val'), ('MongoDB', '- new val'), ('Checking', 'the new val'), ('Customer', 'meeting new val'), ('MongoDB', 'connection new val'), ('MongoDB', 'dll new val'), ('Mongodb', 'data new val'), ('MongoDB', 'OOM new val'), ('Mongodb', 'connection new val'), ('SSH', 'gathering new val'), ('MongoDB', 'customer new val'), ('MongoDB', 'SSH new val'), ('MongoDB', 'SSH new val')]\n",
      "\n",
      "Flap Map Values: [('Mongodb', '- - d a t a c o n n'), ('MongoDB', '- c o n n d l l O O M c u s t S S H S S H'), ('Checking', 't h e'), ('Customer', 'm e e t'), ('SSH', 'g a t h')]\n",
      "\n",
      "Keys:  ['Mongodb', 'Mongodb', 'MongoDB', 'Checking', 'Customer', 'MongoDB', 'MongoDB', 'Mongodb', 'MongoDB', 'Mongodb', 'SSH', 'MongoDB', 'MongoDB', 'MongoDB']\n",
      "\n",
      "Values:  ['-', '-', '-', 'the', 'meeting', 'connection', 'dll', 'data', 'OOM', 'connection', 'gathering', 'customer', 'SSH', 'SSH']\n",
      "\n",
      "Sort By Key:  [('Checking', 'the'), ('Customer', 'meeting'), ('MongoDB', '-'), ('MongoDB', 'connection'), ('MongoDB', 'dll'), ('MongoDB', 'OOM'), ('MongoDB', 'customer'), ('MongoDB', 'SSH'), ('MongoDB', 'SSH'), ('Mongodb', '-'), ('Mongodb', '-'), ('Mongodb', 'data'), ('Mongodb', 'connection'), ('SSH', 'gathering')]\n"
     ]
    }
   ],
   "source": [
    "print('\\nMap Values:', wordpair.mapValues(lambda x: x + \" new val\").collect())\n",
    "print('\\nFlap Map Values:', wordpair.flatMapValues(lambda x: x[0:4]).reduceByKey(lambda x,y: x +\" \"+ y).collect())\n",
    "print('\\nKeys: ', wordpair.keys().collect())\n",
    "print('\\nValues: ', wordpair.values().collect())\n",
    "print('\\nReduce By Key: ', wordpair.reduceByKey(lambda x,y: x +\" \"+ y).collect())\n",
    "print('\\nFold By Key: ', wordpair.foldByKey(\"XX\", lambda x,y: x +\" \"+ y).collect())\n",
    "print('\\nGroup By Key: ', wordpair.groupByKey().collect())\n",
    "print('\\nSort By Key: ', wordpair.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "[('physics', 214), ('maths', 110), ('english', 65)]\n",
      "[('physics', (214, 3)), ('maths', (110, 2)), ('english', (65, 1))]\n"
     ]
    }
   ],
   "source": [
    "#combineByKey\n",
    "print(listrdd.collect())\n",
    "\n",
    "combine = listrdd.combineByKey(lambda v: v, lambda v1,v2: v1+v2, lambda c1,c2: c1+c2)\n",
    "print(combine.collect())\n",
    "\n",
    "def createCombiner(v):   #initial values of each keys only are passed here, not keys\n",
    "    return (v, 1)\n",
    "\n",
    "def mergeValues(v1, v2):  #values are merged here\n",
    "    return (v1[0] + v2, v1[1] + 1)\n",
    "\n",
    "def mergeCombiners(c1, c2):  #values are combined here\n",
    "    return (c1[0] + c2[0], c1[1]+ c2[1])\n",
    "    \n",
    "\n",
    "combine2 = listrdd.combineByKey(createCombiner, mergeValues, mergeCombiners)\n",
    "print(combine2.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('physics', 214), ('maths', 110), ('english', 65)]\n"
     ]
    }
   ],
   "source": [
    "#aggregateByKey\n",
    "seqop = lambda acc, value: (acc + value)\n",
    "combop = lambda acc1, acc2: (acc1 + acc2)  #acc1[0] + acc2[0], acc1[1] + acc2[1]\n",
    "aggbykey = listrdd.aggregateByKey(0, seqop , combop)\n",
    "print(aggbykey.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mongodb', <pyspark.resultiterable.ResultIterable at 0xba56f90>),\n",
       " ('MongoDB', <pyspark.resultiterable.ResultIterable at 0xba56450>),\n",
       " ('Checking', <pyspark.resultiterable.ResultIterable at 0xba561f0>),\n",
       " ('Customer', <pyspark.resultiterable.ResultIterable at 0xba56430>),\n",
       " ('SSH', <pyspark.resultiterable.ResultIterable at 0xba562d0>)]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbk = wordpair.groupByKey()\n",
    "gbk.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subtract By Key:  [('english', 65)]\n",
      "\n",
      "Join:  [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20))]\n",
      "\n",
      "left outer join:  [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20)), ('english', (65, None))]\n",
      "\n",
      "right outer join:  [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20))]\n",
      "\n",
      "cogroup:  [('physics', (<pyspark.resultiterable.ResultIterable object at 0x0B61C1F0>, <pyspark.resultiterable.ResultIterable object at 0x0B61CA70>)), ('maths', (<pyspark.resultiterable.ResultIterable object at 0x0B61CC70>, <pyspark.resultiterable.ResultIterable object at 0x0B61C970>)), ('english', (<pyspark.resultiterable.ResultIterable object at 0x0B61CFD0>, <pyspark.resultiterable.ResultIterable object at 0x0B61C070>))]\n"
     ]
    }
   ],
   "source": [
    "otherListrdd = sc.parallelize(list({(\"maths\", 20),\n",
    "                              (\"maths\", 100),\n",
    "                              (\"physics\", 66), \n",
    "                              (\"physics\", 90)}), \n",
    "                         1)\n",
    "\n",
    "print('\\nSubtract By Key: ', listrdd.subtractByKey(otherListrdd).collect())\n",
    "\n",
    "print('\\nJoin: ', listrdd.join(otherListrdd).collect())\n",
    "\n",
    "print(\"\\nleft outer join: \", listrdd.leftOuterJoin(otherListrdd).collect())\n",
    "\n",
    "print(\"\\nright outer join: \", listrdd.rightOuterJoin(otherListrdd).collect())\n",
    "\n",
    "print(\"\\ncogroup: \",  listrdd.cogroup(otherListrdd).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1: [('physics', 66), ('maths', 50), ('maths', 60), ('physics', 61), ('english', 65), ('physics', 87)]\n",
      "Input2: [('maths', 100), ('physics', 66), ('physics', 90), ('maths', 20)]\n",
      "\n",
      "Join: [('physics', (66, 66)), ('physics', (66, 90)), ('physics', (61, 66)), ('physics', (61, 90)), ('physics', (87, 66)), ('physics', (87, 90)), ('maths', (50, 100)), ('maths', (50, 20)), ('maths', (60, 100)), ('maths', (60, 20))]\n",
      "\n",
      "Join+MapValues: [('physics', 132), ('physics', 156), ('physics', 127), ('physics', 151), ('physics', 153), ('physics', 177), ('maths', 150), ('maths', 70), ('maths', 160), ('maths', 80)]\n",
      "\n",
      "Join+Map+Agg: [('physics', 896), ('maths', 460)]\n"
     ]
    }
   ],
   "source": [
    "#Join + MapValues + AggregateByKey\n",
    "print(\"Input1:\", inputrdd.collect())\n",
    "print(\"Input2:\", otherListrdd.collect())\n",
    "joinrdd = listrdd.join(otherListrdd)\n",
    "print(\"\\nJoin:\", joinrdd.collect())\n",
    "maprdd = joinrdd.mapValues(lambda x: x[0] + x[1])\n",
    "print(\"\\nJoin+MapValues:\", maprdd.collect())\n",
    "joinMapAgg = maprdd.aggregateByKey(0, lambda x,y: x + y, lambda x,y: x + y)\n",
    "print(\"\\nJoin+Map+Agg:\", joinMapAgg.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pair RDD Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count by key:  defaultdict(<class 'int'>, {'Mongodb': 4, 'MongoDB': 7, 'Checking': 1, 'Customer': 1, 'SSH': 1})\n",
      "\n",
      "Collect As Map:  {'Mongodb': 'connection', 'MongoDB': 'SSH', 'Checking': 'the', 'Customer': 'meeting', 'SSH': 'gathering'}\n",
      "\n",
      "Lookup ['-', 'connection', 'dll', 'OOM', 'customer', 'SSH', 'SSH']\n"
     ]
    }
   ],
   "source": [
    "print('\\nCount by key: ',wordpair.countByKey())\n",
    "print('\\nCollect As Map: ', wordpair.collectAsMap()) #if there are multiple keys with different values, then collectASMap() will collect by returning the updated value with respect to the key\n",
    "print('\\nLookup', wordpair.lookup('MongoDB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persisting \n",
    "\n",
    "#word.persist() or word.cache()\n",
    "#word.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partitioning\n",
    "word.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD to Data frame\n",
    "columns = ['title']\n",
    "df = word.toDF(columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
